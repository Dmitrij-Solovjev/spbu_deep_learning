{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f70b4d4-151c-4334-bb2c-b0adb510d2f5",
   "metadata": {},
   "source": [
    "# Энкодер-декодер архитектура\n",
    "Сегодня мы рассмотрим такую важную архитектуру как энкодер-декодер.\n",
    "В контексте обработки естественного языка эта архитекутра используется чаще всего в задачах преобразования последовательности в последовательность (seq2seq). Такие задачи включают, например, машинный перевод. \n",
    "\n",
    "Вопрос: Что еще бывает?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9835bc24-44e3-4c22-9e95-5f6ef4506f9a",
   "metadata": {},
   "source": [
    "Рассмотрим общий вид таких моделей (здесь и далее иллюстрации из курса Лены Войты): \n",
    "\n",
    "![alt_text](../../additional_materials/images/enc_dec-min.png)\n",
    "\n",
    "Мы с вами до сих пор занимались рекуррентными сетями. Мы уже знаем, как модель работает в случае обычного моделирования языка. В случае энкодер-декодера в ней принципиально ничего не меняется - мы все еще будем генерировать предложения токен за токеном. Однако для формирования результата нам нужно уже что-то, описывающее вход. Энкодер-декодер в случае перевода имеет вид что=то вроде следующего:\n",
    "\n",
    "![alt_text](../../additional_materials/images/enc_dec_simple_rnn-min.png)\n",
    "\n",
    "Идея довольно проста - давайте использовать последнее скрытое состояние закодированного входа как начальное для выхода. \n",
    "Вопрос: в чем минус такого решения?\n",
    "Вопрос: а если у нас на входе не текст?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4abe6f-9698-4d47-ab28-6080ac4e500c",
   "metadata": {},
   "source": [
    "Мы опробуем наши модели кодировщика-декодера на проблеме понимания изображений. Эта задача, с одной стороны, заставляет нас использовать соответствующую архитектуру, с другой - позволяет опустить обучение энкодера, сосредоточившись только на декодере. \n",
    "\n",
    "Прежде чем мы перейдем к архитектуре, необходимо выполнить предварительную обработку. У нас есть две части нашей задачи - изображения и их описания. Для текстов нам необходимо применить токенизацию.\n",
    "\n",
    "Однако, в отличие от прошлого раза, мы не будем применять простейшее представление текста. Наши строки данных содержат уникальные редкие слова. Если мы будем действовать на уровне слова, нам придется иметь дело с большим словарным запасом, мучаться с нормализацией и т.д. Если вместо этого использовать модели на уровне символов, то для обработки последовательности потребуется много итераций. На этот раз мы выберем что-нибудь среднее.\n",
    "\n",
    "Один из популярных подходов называется кодированием Byte Pair (BPE). Алгоритм начинается с токенизации на уровне символов, а затем итеративно объединяет наиболее часто встречающиеся пары в течении N итераций. Это приводит к тому, что часто встречающиеся слова объединяются в один символ, а редкие слова разбиваются на слоги или даже символы. С одной стороны, мы отнсительно эффективно составляем словарб, с другой стороны, если мы даже не будем знать новое слово, мы сможем побить его на символы и все равно закодировать."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4239004b-0f0e-4e74-a517-4d4a9f34e7bb",
   "metadata": {},
   "source": [
    "Установим необходиме библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9927362-9e9c-40e0-aab7-c8a0997be9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9784c76e-82b4-47c7-817b-3366c9a65ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install subword_nmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fbeba6-0581-4cb3-9ae2-1abe74a3b3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as pjoin\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from subword_nmt.learn_bpe import learn_bpe\n",
    "from subword_nmt.apply_bpe import BPE\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1a3df4-798d-40f3-acb0-017981a9f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../datasets/flikr\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88fa360-b606-4f15-9c23-6dd36db99d60",
   "metadata": {},
   "source": [
    "Протокенизируем наш датасет. Здесь мы возьмем все текстовые строки из корпуса и составим токены для них.\n",
    "Материал по токенизации: [Byte-Pair Encoding (BPE)](https://huggingface.co/learn/nlp-course/ru/chapter6/5)\n",
    "\n",
    "Вопрос: а как нам пришлось бы токенизировать тексты в случае машинного перевода?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68e570-6402-4523-9542-485c5e4fac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "def tokenize(x):\n",
    "    return ' '.join(tokenizer.tokenize(x.lower()))\n",
    "\n",
    "# разделение описаний картинок на отдельные токены\n",
    "with open('train', 'w') as f_src:\n",
    "    for line in open(pjoin(data_path, 'captions.txt')):\n",
    "        words = line.strip().split(',')\n",
    "        image, src_line = words[0], \",\".join(words[1:])\n",
    "        f_src.write(tokenize(src_line) + '\\n')\n",
    "\n",
    "# build and apply bpe voc\n",
    "learn_bpe(open('./train'), open('bpe_rules', 'w'), num_symbols=8000)\n",
    "bpe = BPE(open('./bpe_rules'))\n",
    "\n",
    "with open('train.bpe', 'w') as f_out:\n",
    "    for line in open('train'):\n",
    "        f_out.write(bpe.process_line(line.strip()) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa22adab-27b1-49c6-bf8f-aa5c0633d012",
   "metadata": {},
   "source": [
    "Теперь нам нужно создать словарь, который сопоставляет строки с токенами и наоборот.Это нам понадобится, когда мы захотим тренировать модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cc8cd3-467d-4602-b379-e32309371487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c7d2f6-4cb4-4ee7-bc94-523ff0169dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inp = np.array(open('./train.bpe').read().split('\\n'))\n",
    "from vocab import Vocab\n",
    "voc = Vocab.from_lines(data_inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2b3bf0-b547-4116-8e4f-5392674bbb57",
   "metadata": {},
   "source": [
    "Наш токенизатор и словарь умеют переводить сразу несколько строк в матрицу токенов, давая результирующий тензор максимальной или заданной длины. Проверим, что все нормально."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e4ccfc-1a4d-437d-97b2-ca31f7162aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's how you cast lines into ids and backwards.\n",
    "batch_lines = sorted(data_inp, key=len)[5:10]\n",
    "batch_ids = voc.to_matrix(batch_lines)\n",
    "batch_lines_restored = voc.to_lines(batch_ids)\n",
    "\n",
    "print(\"lines\")\n",
    "print(batch_lines)\n",
    "print(\"\\nwords to ids (0 = bos, 1 = eos):\")\n",
    "print(batch_ids)\n",
    "print(\"\\nback to words\")\n",
    "print(batch_lines_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e339391b-b8fa-4156-9606-99a42a2dde16",
   "metadata": {},
   "source": [
    "Также мы можем узнать, какое распределение числа токенов на предложение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f79f48-fedd-4a9b-bb63-8bba3976220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8, 4])\n",
    "plt.title(\"caption length\")\n",
    "plt.hist(list(map(len, map(str.split, data_inp))), bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46974a1-0362-4a36-b46a-26e4563dfb74",
   "metadata": {},
   "source": [
    "Как выглядит наш датасет? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72766743-d174-4f53-8917-7ba3832f43d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = pd.read_csv(pjoin(data_path, 'captions.txt')).dropna()\n",
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafba707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#captions[\"caption\"][0]\n",
    "\n",
    "# Here's how you cast lines into ids and backwards.\n",
    "batch_lines = sorted(data_inp, key=len)[5:10] # captions[\"caption\"][0]\n",
    "batch_ids = voc.to_matrix(batch_lines)\n",
    "batch_lines_restored = voc.to_lines(batch_ids)\n",
    "\n",
    "print(\"lines\")\n",
    "print(batch_lines)\n",
    "print(\"\\nwords to ids (0 = bos, 1 = eos):\")\n",
    "print(batch_ids)\n",
    "print(\"\\nback to words\")\n",
    "print(batch_lines_restored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4a8f08-3a5f-4deb-bec8-91e2ffd166bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file = captions[\"image\"].sample(1).iloc[0]\n",
    "# выведите все описания для этой картинки\n",
    "Image.open(pjoin(data_path, \"Images\", image_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d4eaf3-ab9c-4692-8551-1758f0d6a2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = {k: i for i, k in enumerate(captions[\"image\"].unique())}\n",
    "image_list = list(map(lambda x: x[0], sorted(image_ids.items(), key=lambda x: x[1])))\n",
    "captions['image_id'] = captions[\"image\"].map(image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dce9f8b-0cc3-4d0e-bcd2-da8721c11f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "#from torchvision.io import read_image\n",
    "\n",
    "class ImagesDataset(Dataset):\n",
    "    def __init__(self, root, image_list, transform=None):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.image_list = image_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # просто загрузите и трансформируйте картинку\n",
    "        image_path = self.image_list[item]\n",
    "        #image = read_image(self.root + \"/\" + image_path)\n",
    "        image = Image.open(self.root + \"/\" + image_path)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5c4f15-669b-4465-9454-da817c2d6360",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c390509b-b9e4-4785-8e91-b91a72802f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "from torch import nn\n",
    "cnn_model = mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1).eval().to(device)\n",
    "cnn_model.classifier = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52add8ac-1e09-475b-90ab-695577cc101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(224),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = ImagesDataset(root=pjoin(data_path, \"Images\"), image_list=image_list, transform=transform)\n",
    "dataloader = DataLoader(dataset, shuffle=False, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa343f8-8865-4784-a7b5-6c6d793d155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dataset[0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ef185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a8d43b-70f8-4a79-9429-d0de6198b602",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images in tqdm(dataloader):\n",
    "        embeds = cnn_model(images.to(device))\n",
    "        image_embeds += [embeds.cpu()]\n",
    "\n",
    "image_embeds = torch.cat(image_embeds, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1049f96-9bce-4c78-8bc2-d8a189b86554",
   "metadata": {},
   "source": [
    "Протестируем результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a7e659-d3bd-487e-9bb8-136154be6cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1001\n",
    "image = Image.open(pjoin(data_path, \"Images\", image_list[i])).convert('RGB')\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e777069-2649-4ac5-ad52-b5b8fe0b6ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model(transform(image).to(device).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8946792-fd03-4de5-82d2-344ea5920b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeds[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88bf595-9626-46ba-99d1-7e3bcc875d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions[captions.image_id == 1001]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc247fc-027a-4c87-bbd2-9c4e1457f083",
   "metadata": {},
   "source": [
    "Итак, у нас все готово для создания тренировочного датасета: вхожные изображения, эмбеддинги и описания. При этом несколько описаний могут использоваться для одного изображения, поэтому будем делить датасет именно по описаниям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0161da10-36ba-46dc-af7a-7e033165a85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#train_images, test_images, train_embeds, test_embeds =  # разделите датасет\n",
    "\n",
    "\n",
    "train_embeds, test_embeds = train_test_split(image_embeds, shuffle=False)\n",
    "train_images = test_images = image_list\n",
    "\n",
    "train_embeds.shape, test_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b0005d-2236-4412-89d1-a4ec0f15b42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions = captions[captions[\"image\"].isin(train_images)]\n",
    "test_captions = captions[captions[\"image\"].isin(test_images)]\n",
    "\n",
    "train_captions.shape, test_captions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd43845b-53df-4302-9ba0-02033ba0de4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionsDataset(Dataset):\n",
    "    def __init__(self, captions, embeds, images, max_len=64):\n",
    "        super().__init__()\n",
    "        self.captions = captions.reset_index()\n",
    "        self.images = images\n",
    "        self.embeds = embeds\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image_id = self.captions.loc[item, \"image_id\"]\n",
    "        return { \"x\": self.embeds[image_id], \"y\": self.captions.loc[item, \"caption\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42ead0d-2562-41bd-9519-d6f05bf4da1b",
   "metadata": {},
   "source": [
    "В этот раз мы попробуем написать свою кастомную функцию склейки в батч с использованием словаря. Она берет элементы батча (список словарей)) и должна объединить x и y в тензоры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f196c35d-5da5-48b9-8707-e53c947e2164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_collate_fn(batch):\n",
    "    # your code \n",
    "    return {\n",
    "        \"x\": torch.stack([batch_i['x'] for batch_i in batch], dim=0), #\n",
    "        \"y\": [batch_i['y'] for batch_i in batch]  #\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603f9ca6-1d07-4073-bc45-77cf4e335ded",
   "metadata": {},
   "source": [
    "Теперь ее достаточно передать в даталоадер при инициализации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaee8b80-d444-45c6-85a7-5c7d1fb8078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CaptionsDataset(train_captions, image_embeds, image_list)\n",
    "test_set = CaptionsDataset(test_captions, image_embeds, image_list)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, collate_fn=rnn_collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=False, collate_fn=rnn_collate_fn)\n",
    "embeds_loader = DataLoader(test_embeds, batch_size=128, shuffle=False, collate_fn=rnn_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c8aa34-32e8-4567-9a33-bdd0f2fc2dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b5f998-b0f3-46fd-afb4-8c6eccced58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2915a5f-131f-49fa-be0b-adb400ba33a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd619e1-479d-4066-b49e-3fc12f41281a",
   "metadata": {},
   "source": [
    "Теперь займемся моделью. Она должна содержать как энкодер, так и декодер. Энкодер должен преобразовывать входной эмбеддинг в понятное модели начальное состояние (скрытое представление и вход), а декодер должен, начиная с этого состояния, генерировать последовательность. Обратите внимание, что если слоев у LSTM больше одного, то инициализировать мы должны все слои сразу. \n",
    "\n",
    "Здесь мы также использоуем слой эмбеддинга, который позволит нам перевести токены в векторы.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e570cad-2f12-48db-b790-3f282e7153ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionRNN(nn.Module):\n",
    "    def __init__(self, image_embed_dim, vocab_size, pad_index=1, eos_index=-1, embed_dim=256, hidden_dim=256, lstm_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.image_embed_to_h0 = # your code\n",
    "        self.image_embed_to_c0 = # your code\n",
    "        self.embedding = # your code\n",
    "        self.lstm = # your code\n",
    "        self.linear = # your code\n",
    "        self.eos_index = eos_index\n",
    "        self.pad_index = pad_index \n",
    "\n",
    "    def forward(self, tokens, image_embeds):\n",
    "        '''\n",
    "        B - batch size\n",
    "        M - lstm layers\n",
    "        L - sequence length\n",
    "        I - image embedding dim\n",
    "        E - embedding dim\n",
    "        H - hidden dim\n",
    "        V - vocab size\n",
    "        '''\n",
    "        # image_embeds: (B, I)\n",
    "        B = # your code\n",
    "        h0 = # your code\n",
    "        c0 = # your code\n",
    "        # h0, co: (M, B, H)\n",
    "\n",
    "        # tokens: (B, L)\n",
    "        # embeds: (B, L, E)\n",
    "        # output: (B, L, H), (h, c): (M, B, H)\n",
    "        # logits: (B, L, V)\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inference(self, image_embeds):\n",
    "        self.eval()\n",
    "        # generate lstm input\n",
    "        B = image_embeds.shape[0]\n",
    "        h = # your code\n",
    "        c = # your code\n",
    "        h, c = h.contiguous(), c.contiguous()\n",
    "\n",
    "        # init tokens with <bos>\n",
    "           # your code\n",
    "        # 2 stopping conditions: reaching max len or getting <eos> token\n",
    "        while tokens.shape[1] < 64:\n",
    "            if ((tokens == self.eos_index).sum(1) > 0).all():\n",
    "                break\n",
    "\n",
    "            # process newly obtained token\n",
    "            # your code\n",
    "            logits = # your code\n",
    "\n",
    "            # get new tokens from logits\n",
    "            new_tokens = logits.argmax(dim=-1)\n",
    "            tokens = torch.cat([tokens, new_tokens], dim=1)\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aa7c6e-79dc-4f57-b852-8a22fc6c8b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CaptionRNN(image_embeds.shape[1], vocab_size=len(voc), eos_index=voc.eos_ix, pad_index=voc.unk_ix)\n",
    "\n",
    "for batch in train_loader:\n",
    "    break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50222091-0d5e-44b2-8afc-52e9717bb928",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efdce95-68b5-48b9-851f-d81f9a33a061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "\n",
    "logits = model(batch[\"y\"], batch[\"x\"])\n",
    "\n",
    "tokens = model.inference(embeds)\n",
    "voc.to_lines(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2964c1d-b772-4de6-9649-a0ecc479ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c68352-a3ea-4bbb-9075-b987aafe2d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461b7936-583c-4f53-90b3-4d57ec013cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "BLEU_FREQ = 5\n",
    "\n",
    "\n",
    "def plot_losses(train_losses, test_losses, test_blues):\n",
    "    clear_output()\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(13, 4))\n",
    "    axs[0].plot(range(1, len(train_losses) + 1), train_losses, label='train', color='deepskyblue', linewidth=2)\n",
    "    axs[0].plot(range(1, len(test_losses) + 1), test_losses, label='test', color='springgreen', linewidth=2)\n",
    "    axs[0].set_ylabel('loss')\n",
    "\n",
    "    axs[1].plot(BLEU_FREQ * np.arange(1, len(test_blues) + 1), test_blues, label='test',\n",
    "                color='springgreen', linewidth=2)\n",
    "    axs[1].set_ylabel('BLEU')\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.set_xlabel('epoch')\n",
    "        ax.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5afe397-c898-49af-b81d-20e4c7fdc041",
   "metadata": {},
   "source": [
    "Напишем трейн луп. Он похож на то, что мы делали раньше, однако мы не используем пакинг. Допольнительное задание: напишите лосс функцию для нашей задачи вручную.\n",
    "\n",
    "Для оценки модели мы попробуем использовать BLEU.Эта метрика была создана для машинного перевода, но может использоваться и длядругих приложений. Эта метрика просто вычисляет, какая часть предсказанных n-грамм действительно присутствует в эталонном переводе. Он делает это для n=1,2,3 и 4 и вычисляет среднее геометрическое со штрафом, если перевод короче эталонного.\n",
    "\n",
    "Хотя BLEU имеет множество недостатков, он по-прежнему остается наиболее часто используемой метрикой и одной из самых простых для расчета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcd64ae-da9d-4ef3-b2fe-33452a375980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import BLEUScore\n",
    "\n",
    "\n",
    "def training_epoch(model, optimizer, criterion, train_loader, tqdm_desc):\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader, desc=tqdm_desc):\n",
    "        # your code\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * embeds.shape[0]\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation_epoch(model, criterion, valid_loader, tqdm_desc):\n",
    "    valid_loss = 0.0\n",
    "    model.eval()\n",
    "    for batch in tqdm(valid_loader, desc=tqdm_desc):\n",
    "        # your code\n",
    "        valid_loss += loss.item() * embeds.shape[0]\n",
    "\n",
    "    valid_loss /= len(valid_loader.dataset)\n",
    "    return valid_loss\n",
    "\n",
    "\n",
    "def evaluate_bleu(model, embeds_loader):\n",
    "    bleu = BLEUScore()\n",
    "    # your code\n",
    "    return bleu(# your code).item()\n",
    "\n",
    "\n",
    "def train(model, optimizer, scheduler, criterion, train_loader, valid_loader, num_epochs, log_frequency=1):\n",
    "    train_losses, valid_losses, valid_blues = [], [], []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = training_epoch(\n",
    "            model, optimizer, criterion, train_loader,\n",
    "            tqdm_desc=f'Training {epoch}/{num_epochs}'\n",
    "        )\n",
    "        valid_loss = validation_epoch(\n",
    "            model, criterion, valid_loader,\n",
    "            tqdm_desc=f'Validating {epoch}/{num_epochs}'\n",
    "        )\n",
    "\n",
    "        if epoch % log_frequency == 0:\n",
    "            valid_bleu = evaluate_bleu(model, valid_loader)\n",
    "            valid_blues += [valid_bleu]\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        train_losses += [train_loss]\n",
    "        valid_losses += [valid_loss]\n",
    "        plot_losses(train_losses, valid_losses, valid_blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805520b1-8614-451c-bc47-c222216f317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 200\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "scheduler = None\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=voc.unk_ix)\n",
    "\n",
    "sum(param.numel() for param in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afeac86-545c-4ac4-b358-6180b524dd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer, scheduler, criterion, train_loader, test_loader,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be429b9-545d-42c5-8bbc-3b2ee9acd2eb",
   "metadata": {},
   "source": [
    "Также посмотрим на случайную картинку из нашего набора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6158da-79e4-40eb-a3fa-bca9bff2646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_random_test_image():\n",
    "    index = np.random.randint(len(test_images))\n",
    "    image_file = test_images[index]\n",
    "    tokens = model.inference(image_embeds[index].unsqueeze(0).to(device)).cpu()\n",
    "    prediction = voc.to_lines(tokens)\n",
    "    print('Prediction:', prediction)\n",
    "\n",
    "    for i, caption in enumerate(captions[captions[\"image\"] == image_file].caption):\n",
    "        print(f'GT caption #{i + 1}:', caption)\n",
    "\n",
    "    return Image.open(pjoin(data_path, 'Images', image_file)).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911d0814-5cfe-42eb-8153-a3a1aad57001",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "caption_random_test_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5cba15-f0fb-4ca9-ba2d-8beb0d089e7c",
   "metadata": {},
   "source": [
    "Что дальше? \n",
    "\n",
    "- Вы можете использовать механизм внимания, чтобы модель была лучше интерпреируема и была качественней.\n",
    "Как это работает: https://distill.pub/2016/augmented-rnns/\n",
    "Один из способов сделать это: https://arxiv.org/abs/1502.03044.\n",
    "- Можно перейти на трансформеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09648aee-720a-4d20-aed3-219eb4bd9fef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
