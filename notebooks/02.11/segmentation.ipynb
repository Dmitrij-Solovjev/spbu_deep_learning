{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44a2d824-1692-4c75-b96b-e9fed55ba330",
   "metadata": {},
   "source": [
    "# Сегментация\n",
    "В этот раз мы рассмотрим чуть менее стандартную задачу - сегментации изображений.\n",
    "[Kaggle](https://www.kaggle.com/datasets/kooaslansefat/uav-segmentation-aeroscapes/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e4742d-cb19-40cf-99d0-ff48ecafd55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import tqdm\n",
    "import wandb\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchmetrics\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import albumentations.pytorch\n",
    "from glob import glob\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from torchvision.transforms import v2 as T\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f4869b-6e3e-4052-8f0a-1a551b3618f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = r\"../../datasets/aeroscapes\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dca0f3-2e65-42e5-b911-c4dc52f5b9db",
   "metadata": {},
   "source": [
    "Для начала рассмотрим датасет изображений, полученных с дрона на высоте от 5 до 50 метров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c42cd-e15b-4a8f-8a1a-c10f04823853",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_NAMES = ['Background', 'Person', 'Bike', 'Car', 'Drone', 'Boat', 'Animal', 'Obstacle', 'Construction', 'Vegetation', 'Road', 'Sky']\n",
    "\n",
    "COLOR_MAPPING = {\n",
    "    (0, 0, 0): 0,            # Background \n",
    "    (192, 128, 128): 1,      # Person\n",
    "    (0, 128, 0): 2,          # Bike\n",
    "    (128, 128, 128): 3,      # Car\n",
    "    (128, 0, 0): 4,          # Drone\n",
    "    (0, 0, 128): 5,          # Boat\n",
    "    (192, 0, 128): 6,        # Animal\n",
    "    (192, 0, 0): 7,          # Obstacle\n",
    "    (192, 128, 0): 8,        # Construction\n",
    "    (0, 64, 0): 9,           # Vegetation\n",
    "    (128, 128, 0): 10,       # Road\n",
    "    (0, 128, 128): 11,       # Sky\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17b4d2b-6f07-4a43-952d-973b0cdcf3c3",
   "metadata": {},
   "source": [
    "Сегментация - это та же классификация. Разница лишь в том, что классифицируется каждый пиксель изображения. Поэтому вместо приятной структуры с папками нам придется вернуться к рукописным датасетам.\n",
    "\n",
    "Создадим датасет: он будет читать файлы из папок с изображениями и масками. Маски трехмерные, но мы знаем их маппинг в классы. Так что легко можно поменять маску на номер желаемого класса. Остальное стандартно - трансформируем семпл, возвращаем результат.\n",
    "\n",
    "Сейчас немного усложним задачу, будем читать из папки Visualizations, а не SegmentationMasks (но вы можете и оттуда сразу читать, конечно)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb41de1-e050-42cd-9bbd-cc30671226ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AerialSegmentationDataset(Dataset):\n",
    "    def __init__(self, root, data = \"trn\", transformations = None):\n",
    "        files = open(f\"{root}/ImageSets/{data}.txt\", \"r\").read().split(\"\\n\")\n",
    "        self.im_paths, self.gt_paths = self.get_data_paths(root = root, files = files) \n",
    "        self.transformations = transformations\n",
    "        self.n_cls = 11\n",
    "        print(len(self.im_paths), len(self.gt_paths))\n",
    "        assert len(self.im_paths) == len(self.gt_paths)\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.im_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        im = self.read_image(self.im_paths[idx])\n",
    "        mask = cv2.imread(self.gt_paths[idx], cv2.IMREAD_COLOR)\n",
    "        mask = convert_mask(mask)\n",
    "        if self.transformations: \n",
    "            # Проведите трансформации\n",
    "            transformed = self.transformations(image=im, mask=mask )\n",
    "            im, mask = transformed[\"image\"], transformed[\"mask\"]\n",
    "        return im, mask\n",
    "        \n",
    "    def get_data_paths(self, root, files) -> tuple[list[str], list[str]]: \n",
    "        images = [\n",
    "            path for path in sorted(glob(f\"{root}/JPEGImages/*.jpg\")) if self.get_fname(path) in files]\n",
    "        masks =  [\n",
    "            path for path in sorted(glob(f\"{root}/Visualizations/*.png\")) if self.get_fname(path) in files]\n",
    "        return images, masks \n",
    "       \n",
    "    def get_fname(self, path): \n",
    "        return os.path.splitext(os.path.basename(path))[0]\n",
    "\n",
    "    def read_image(self, path): \n",
    "        return cv2.cvtColor(\n",
    "            cv2.imread(path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB\n",
    "        )\n",
    "\n",
    "def convert_mask(mask_rgb):\n",
    "    mask = np.zeros((mask_rgb.shape[0], mask_rgb.shape[1]), dtype=np.uint8)\n",
    "    for rgb, idx in COLOR_MAPPING.items():\n",
    "        mask[(mask_rgb == rgb).all(axis=2)] = idx\n",
    "    return mask\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7b0aa9-3050-4201-a021-caad827a495d",
   "metadata": {},
   "source": [
    "Создадим инстанс датасета для визуализации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d942f51b-b477-43be-b763-5e5d7c562a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_h, im_w = 256, 256\n",
    "# mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225] # стандарт\n",
    "#transformations = A.Compose( [A.Resize(im_h, im_w),ToTensorV2(transpose_mask = True) ])\n",
    "#transformations = A.Compose( [\n",
    "#    A.Resize(im_h, im_w),\n",
    "#    A.HorizontalFlip(p=0.5),\n",
    "#    A.RandomBrightnessContrast(p=0.2),\n",
    "#    A.augmentations.transforms.Normalize(mean = mean, std = std),\n",
    "#    ToTensorV2(transpose_mask = True)\n",
    "#])\n",
    "\n",
    "transformations = A.Compose( [\n",
    "    A.Resize(im_h, im_w),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.ToFloat(),\n",
    "#    A.augmentations.transforms.Normalize(), # Из-за нормализации картинка превращается в пиксельную кашу\n",
    "    ToTensorV2(transpose_mask = True)\n",
    "])\n",
    "#standart_transform = True\n",
    "#standart_transform = True\n",
    "\n",
    "train_dataset = AerialSegmentationDataset(root=base_dir, data=\"trn\", transformations=transformations)\n",
    "test_dataset = AerialSegmentationDataset(root=base_dir, data=\"val\", transformations=transformations)\n",
    "n_cls = train_dataset.n_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e62e69e-c1f8-4edb-87db-2e09d2af0e89",
   "metadata": {},
   "source": [
    "Добавим функции для того, чтобы красиво нарисовать наши примеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e087bc-cbfc-43ff-85b5-d610d6c4b10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(rows, cols, count, im, gt = None, title = \"Original Image\"):\n",
    "    plt.subplot(rows, cols, count)\n",
    "    image = im.squeeze(0).int().permute(1,2,0) if not gt else im.squeeze(0).float()\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\"); plt.title(title)\n",
    "    return count + 1\n",
    "    \n",
    "def visualize(ds, n_ims):\n",
    "    plt.figure(figsize = (25, 20))\n",
    "    rows = n_ims // 4; cols = n_ims // rows\n",
    "    count = 1\n",
    "    indices = [random.randint(0, len(ds)) for _ in range(n_ims)]\n",
    "    for idx, index in enumerate(indices):\n",
    "        if count == n_ims + 1: break\n",
    "        im, gt = ds[index]\n",
    "        # First Plot\n",
    "        count = plot(rows, cols, count, im = im*255)\n",
    "        # Second Plot\n",
    "        count = plot(rows, cols, count, im = gt, gt = True, title=\"Segmentation mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7943bbf9-ed4e-4c39-838e-2beefeb19c6b",
   "metadata": {},
   "source": [
    "И нарисуем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1b4514-aec7-4f78-a55d-a9a9a0d611ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(train_dataset, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8685a723-2e7b-49a8-a9a7-416a40698b71",
   "metadata": {},
   "source": [
    "Дополнительное задание: выведите исходные пары изображение - трехканальная маска"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f111051d-e150-49f0-a29f-a98a3a441c74",
   "metadata": {},
   "source": [
    "Теперь уже создадим наш датасет, а также разобьем его на трейн-валидацию-тест\n",
    "\n",
    "**Допзадание**: попробуйте еще другие аугментации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd742d00-6dc5-49fc-9526-268a1747d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "transformations = A.Compose( [\n",
    "    A.Resize(im_h, im_w),\n",
    "    A.augmentations.transforms.Normalize(mean = mean, std = std), \n",
    "    ToTensorV2(transpose_mask = True)\n",
    "])\n",
    "\n",
    "split = [0.9, 0.1]\n",
    "batch_size = 16\n",
    "\n",
    "# создайте датасеты для теста и валидации\n",
    "train_len = int(len(train_dataset)*split[0]) # Определите обучающую длину\n",
    "val_len = int(len(train_dataset)*split[1])+1 # Определите валидационную длину\n",
    "\n",
    "# Data split\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_len, val_len])\n",
    "    \n",
    "print(f\"\\nThere are {len(train_dataset)} number of images in the train set\")\n",
    "print(f\"There are {len(val_dataset)} number of images in the validation set\")\n",
    "print(f\"There are {len(test_dataset)} number of images in the test set\\n\")\n",
    "\n",
    "# Get dataloaders\n",
    "train_dataloader  = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle = True)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle = False)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=1, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66844c2f-9085-49bd-abce-527229463441",
   "metadata": {},
   "source": [
    "Теперь создадим нашу архитектуру. Это [UNet](https://arxiv.org/pdf/1505.04597) - одна из базовых архитектур для сегментацию, аналоги которой еще и активно используются в самых разных приложениях.\n",
    "\n",
    "Задание: дополните код, чтобы получить UNet с двумя внутренними блоками. \n",
    "Минимальная задача для слабых компьютеров - максимальное число фичей - 128. В таком случае мы будем использовать только по два блока, а не 4 исходных.\n",
    "\n",
    "Исходная архитектура:\n",
    "![alt_text](../../additional_materials/images/u-net-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa5d2f2-3965-4c46-9e49-350f3c537ec9",
   "metadata": {},
   "source": [
    "# Как реализовать up-conv?\n",
    "В torch для этого есть nn.TransposeConv2d. \n",
    "На самом деле это не совсем свертка - она берет входные значения и вначале создает что-то вроде буфера, где хранит результаты умножения ядра на вход поэлементно. После того, как все результаты получены, они собираются воедино суммированием (можно посмотреть подробное объяснение [здесь](https://stackoverflow.com/questions/69782823/understanding-the-pytorch-implementation-of-conv2dtranspose):\n",
    "\n",
    "![alt_text](../../additional_materials/images/tc1.png)\n",
    "\n",
    "Примеры:\n",
    "\n",
    "![alt_text](../../additional_materials/images/tc2.png)\n",
    "\n",
    "![alt_text](../../additional_materials/images/tc3.png)\n",
    "\n",
    "![alt_text](../../additional_materials/images/tc4.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c823fb1-793b-4b01-b56f-47c49b1e170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    # YOUR CODE OF BLOCK\n",
    "    # Note that the hxw must stay the same\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1)\n",
    "        #self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x).relu()\n",
    "        x = self.conv2(x).relu()\n",
    "        #x = self.conv3(x).relu()\n",
    "        return x\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 3,\n",
    "        out_channels: int = 1,\n",
    "        features: List[int] = [32, 64, 128],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.down = nn.ModuleList()\n",
    "        self.up = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(2, 2) # Phase change - from down to up\n",
    "        # append blocks:\n",
    "            # append blocks to down part\n",
    "            # features x2 in out\n",
    "        last_features = in_channels\n",
    "        for num_features in features:\n",
    "            self.down.append(ConvBlock(last_features, num_features))\n",
    "            last_features = num_features\n",
    "        #append blocks in reversed order:\n",
    "            # append pairs of [transposed convolutions and blocks to up)\n",
    "            # features /2 in out\n",
    "        \n",
    "        for num_features in reversed(features):\n",
    "            self.up.append(nn.ConvTranspose2d(num_features * 2, num_features, kernel_size=2, stride=2))\n",
    "            self.up.append(ConvBlock(num_features * 2, num_features))\n",
    "\n",
    "        self.bottleneck = ConvBlock(last_features, last_features * 2) # your code\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1) # your code\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        skip_connections = []  # storage for skip connections\n",
    "        for down in self.down:\n",
    "            # add forwards and also fill skip connections\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "        \n",
    "        x = self.bottleneck(x) # add bottleneck\n",
    "        skip_connections = skip_connections[::-1]# reverse list of skip connections\n",
    "        for idx in range(0, len(self.up), 2):\n",
    "            # run up and also get corresponding skip\n",
    "            x = self.up[idx](x)\n",
    "            skip = skip_connections[idx // 2]\n",
    "            # if shape of x and skipped x are different, don't forget to reshape x\n",
    "            if x.shape != skip.shape:\n",
    "                # transforms.Resize()\n",
    "                skip = transforms.Resize(skip, x.shape[-2:])\n",
    "            \n",
    "            concat_skip = torch.cat((skip, x), dim=1) # Concat skip and x along channels dimensions (b, c, h, w)\n",
    "            x = self.up[idx + 1](concat_skip) # call next block with skip\n",
    "        return self.final_conv(x) # call final conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09fac1f-4b3f-4990-9dec-b966b21a2e51",
   "metadata": {},
   "source": [
    "Создадим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba4a892-c485-44cd-87f0-fd190f20add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(out_channels=n_cls+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a51c37-a9bc-44e2-94e2-ae96a18659b1",
   "metadata": {},
   "source": [
    "Теперь немного подушним с метриками. Для сегментации и детекции они похожи, так как обе задачи требуют определения целевых регионов. Поэтому наиболее естественны метрики, основанные на геометрии.\n",
    "Рассмотрим две: IoU и Dice. \n",
    "\n",
    "![alt_text](../../additional_materials/images/metrics.png)\n",
    "\n",
    "Так как у нас задача мультиклассовая, мы должны посчитать эти метрики по классам и усреднить результат.\n",
    "\n",
    "**Задание**: Дополните код, реализовав метрики mIoU и Dice.\n",
    "\n",
    "\n",
    "Для того, чтобы можно было в будущем сочетать наши рукописные метрики и торчовые, напишем обертку, которая хранит в себе словарь со всем метриками сразу. Каждая рукописная должна иметь те же методы, что и torchmetrics.Metric: __call__ и compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8311f33b-26f8-47f9-90f3-daaf18622983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Iterable, Dict\n",
    "\n",
    "class Metric(ABC):\n",
    "    def __init__(self):\n",
    "        self._values = []\n",
    "\n",
    "    def __call__(self, preds: Iterable, targets: Iterable) -> float:\n",
    "        value = self.compute_metric(preds, targets)\n",
    "        self._values.append(value)\n",
    "        return value\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_metric(self, preds: Iterable, targets: Iterable) -> float:\n",
    "        pass\n",
    "\n",
    "    def compute(self) -> float:\n",
    "        if not self._values:\n",
    "            return 0.0\n",
    "        mean_value = sum(self._values) / len(self._values)\n",
    "        return mean_value\n",
    "\n",
    "    def reset(self):\n",
    "        self._values = []\n",
    "\n",
    "\n",
    "class MetricCollection:\n",
    "    def __init__(self, metrics: Dict[str, Metric | torchmetrics.Metric]):\n",
    "        if not isinstance(metrics, dict):\n",
    "            raise TypeError(\"metrics must be a dictionary of Metric instances.\")\n",
    "        for name, metric in metrics.items():\n",
    "            if not isinstance(metric, Metric) and not isinstance(metric, torchmetrics.Metric):\n",
    "                raise TypeError(\n",
    "                    f\"Value for '{name}' is not an instance of Metric (handmade or torchmetrics).\"\n",
    "                )\n",
    "        self.metrics = metrics\n",
    "\n",
    "    def __call__(self, preds: Iterable, targets: Iterable) -> Dict[str, float]:\n",
    "        ''' Call metrics one by one ''' \n",
    "        computed_values = {}\n",
    "        for name, metric in self.metrics.items():\n",
    "            computed_values[name] = metric(preds, targets)\n",
    "        return computed_values\n",
    "\n",
    "    def compute(self) -> Dict[str, float]:\n",
    "        ''' compute metrics one by one '''\n",
    "        computed_values = {}\n",
    "        for name, metric in self.metrics.items():\n",
    "            computed_values[name] = metric.compute()\n",
    "        return computed_values\n",
    "\n",
    "    def reset(self):\n",
    "        ''' reset metrics one by one '''\n",
    "        for metric in self.metrics.values():\n",
    "            metric.reset()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15f1032-b2b5-4bf4-ba05-275fd6c0b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanIntersectionOverUnion(Metric):\n",
    "    def __init__(self, num_classes: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.eps = eps\n",
    "\n",
    "    def compute_metric(self, preds: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "        with torch.no_grad():\n",
    "            iou_per_class = []\n",
    "            for c in range(1, self.num_classes):\n",
    "                pred_inds = preds == c\n",
    "                targets_inds = targets == c\n",
    "                intersection = (pred_inds[targets_inds]).long().sum().data.cpu().item()\n",
    "                union = pred_inds.long().sum().data.cpu().item() + targets_inds.long().sum().data.cpu().item() - intersection\n",
    "                if union == 0:\n",
    "                    iou_per_class.append(np.nan)\n",
    "                else:\n",
    "                    iou_per_class.append(float(intersection) / (float(max(union, 1)) + self.eps))\n",
    "                # compute IoU\n",
    "        return np.nanmean(iou_per_class)\n",
    "\n",
    "\n",
    "class DiceScore(Metric):\n",
    "    def __init__(self, num_classes: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.eps = eps\n",
    "\n",
    "    def compute_metric(self, preds: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "        with torch.no_grad():\n",
    "            # If preds are in one-hot format, convert them to class indices\n",
    "            if preds.dim() == targets.dim() + 1:\n",
    "                preds = torch.argmax(preds, dim=1)\n",
    "\n",
    "            iou_per_class = []\n",
    "            for c in range(self.num_classes):\n",
    "                pred_inds = preds == c\n",
    "                target_inds = targets == c\n",
    "\n",
    "                intersection = (pred_inds & target_inds).sum().float().cpu().item()\n",
    "                pred_sum = pred_inds.sum().float().cpu().item()\n",
    "                target_sum = target_inds.sum().float().cpu().item()\n",
    "\n",
    "                denominator = pred_sum + target_sum\n",
    "                if denominator == 0:\n",
    "                    # If there is no ground truth and no prediction for the class, consider it as NaN\n",
    "                    iou_per_class.append(np.nan)\n",
    "                else:\n",
    "                    dice = (2.0 * intersection + self.eps) / (denominator + self.eps)\n",
    "                    iou_per_class.append(dice)\n",
    "\n",
    "            mean_dice = np.nanmean(iou_per_class)\n",
    "            return mean_dice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d5ef8d-bad4-49c0-b724-e94f46fac379",
   "metadata": {},
   "source": [
    "Зададим лосс и оптимизатор. У нас еще задача не очень сложная, можем воспользоваться кросс-энтропией.\n",
    "\n",
    "**Доп.задание**: создайте лосс из Dice Score. Обучите модель с новым лоссом. Как меняется результат? Объясните, почему можно сделать лосс из Dice Score, но не выйдет из IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf80170b-5140-4c60-a9f3-76a595f4ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "            model.parameters(), lr=0.0005, weight_decay=0.3\n",
    "        )\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8914657a-4779-4b21-bc29-2b1062e0e8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MetricCollection({\"mIoU\": MeanIntersectionOverUnion(n_cls), \"dice\": DiceScore(n_cls)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7b1919-223a-436d-bd3a-ada4188a1138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    metrics: MetricCollection, \n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    binary: int | None = None,\n",
    "    return_losses=False,\n",
    "    scheduler: torch.optim.lr_scheduler = None\n",
    "):\n",
    "    model = model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    all_losses = []\n",
    "    with tqdm.tqdm(total=len(data_loader)) as prbar:\n",
    "        for inputs, targets in data_loader:\n",
    "            metrics.reset()\n",
    "            # YOUR CODE\n",
    "            # train step: preds, loss, optimization, etc            \n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            preds = model(inputs)\n",
    "            loss = criterion(preds, targets.long())\n",
    "            metrics_dict = metrics(torch.max(preds, dim=1)[1], targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            metrics_desc = \",\".join([f\"{i}: {v:.4}\" for i,v in metrics_dict.items()])\n",
    "            # update description for tqdm\n",
    "            prbar.set_description(\n",
    "                f\"Loss: {round(loss.item(), 4)} LR: {scheduler.get_last_lr()[0]}  \" + metrics_desc\n",
    "            )\n",
    "            prbar.update(1)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            all_losses.append(loss.detach().item())\n",
    "            scheduler.step(loss.item())\n",
    "    metric_values = metrics.compute()\n",
    "    total_loss /= len(data_loader)\n",
    "    if return_losses:\n",
    "        return {**metric_values, \"loss\": total_loss}, all_losses\n",
    "    else:\n",
    "        return {**metric_values, \"loss\": total_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeb987c-a577-43ec-baf4-c5950d6f5a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "        model: nn.Module,\n",
    "        data_loader: torch.utils.data.DataLoader,\n",
    "        criterion: nn.Module,\n",
    "        metrics: MetricCollection,\n",
    "        binary: bool | None = None,\n",
    "        device: str = \"cuda:0\"):\n",
    "    model = model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    with tqdm.tqdm(total=len(data_loader)) as prbar:\n",
    "        for inputs, targets in data_loader:\n",
    "            metrics.reset()\n",
    "            with torch.no_grad():\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                preds = model(inputs)\n",
    "                loss = criterion(preds, targets.long())\n",
    "                metrics_dict = metrics(torch.max(preds, dim=1)[1], targets)\n",
    "            metrics_desc = \",\".join([f\"{i}: {v:.4}\" for i,v in metrics_dict.items()])\n",
    "            prbar.set_description(\n",
    "                f\"Loss: {loss.item():.4} \" + metrics_desc\n",
    "            )\n",
    "            prbar.update(1)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        total_loss /= len(data_loader)\n",
    "    return {**metrics.compute(), \"loss\": total_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd7a2ad-6489-415e-b887-a71422c844ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    epochs: int,\n",
    "    train_data_loader: torch.utils.data.DataLoader,\n",
    "    validation_data_loader: torch.utils.data.DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    metrics: MetricCollection,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler=None,\n",
    "    binary: int | None = None,\n",
    "    device=\"cuda:0\",\n",
    "):\n",
    "    all_train_losses = []\n",
    "    epoch_train_losses = []\n",
    "    epoch_eval_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        # construct iterators\n",
    "        train_iterator = iter(train_data_loader)\n",
    "        validation_iterator = iter(validation_data_loader)\n",
    "        # train step\n",
    "        print(f\"Train Epoch: {epoch}\")\n",
    "        train_metrics, one_epoch_train_losses = train_epoch(\n",
    "            model=model,\n",
    "            data_loader=train_iterator,\n",
    "            criterion=criterion,\n",
    "            metrics=metrics,\n",
    "            optimizer=optimizer,\n",
    "            binary=binary,\n",
    "            return_losses=True,\n",
    "            scheduler=scheduler\n",
    "        )\n",
    "        # save train losses\n",
    "        all_train_losses.extend(one_epoch_train_losses)\n",
    "        epoch_train_losses.append(train_metrics[\"loss\"])\n",
    "        # eval step\n",
    "        print(f\"Validation Epoch: {epoch}\")\n",
    "\n",
    "        # call function validation epoch and\n",
    "        # save eval losses\n",
    "        with torch.no_grad():\n",
    "            validation_metrics = validate(\n",
    "                model=model,\n",
    "                data_loader=validation_iterator,\n",
    "                criterion=criterion,\n",
    "                metrics=metrics,\n",
    "                binary=binary,\n",
    "                device=device\n",
    "            )\n",
    "        # save eval losses\n",
    "        epoch_eval_losses.append(validation_metrics[\"loss\"])\n",
    "    return [all_train_losses, epoch_train_losses, epoch_eval_losses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a263328b-648c-4c8a-8bed-0f9d759faca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "all_train_losses1, epoch_train_losses1, epoch_eval_losses1 = train(\n",
    "    model=model,\n",
    "    epochs=5,\n",
    "    train_data_loader=train_dataloader,\n",
    "    validation_data_loader=val_dataloader,\n",
    "    criterion=loss,\n",
    "    metrics=metrics,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    #num_classes=n_cls\n",
    "    scheduler=scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d30ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].plot(all_train_losses1)\n",
    "axes[0].set_title('All Train Losses')\n",
    "axes[1].plot(epoch_train_losses1)\n",
    "axes[1].set_title('Epoch Train Losses')\n",
    "axes[2].plot(epoch_eval_losses1)\n",
    "axes[2].set_title('Epoch Eval Losses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f2f852-e8d4-4a59-9a4a-f72387a3ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_img(\n",
    "    net: nn.Module, img: torch.Tensor, device: str = device, out_threshold: float = 0.5\n",
    ") -> np.ndarray:\n",
    "    net.eval()\n",
    "    net.to(device)\n",
    "    \n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.to(device=device, dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = net(img)\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        full_mask = probs.cpu().squeeze()\n",
    "\n",
    "        return (full_mask > out_threshold) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01f4223-2791-4d8b-a782-2916dd08d435",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_dice = 0\n",
    "avg_mIoU = 0\n",
    "for image, target in test_dataset:\n",
    "    print(\".\", end =\"\")\n",
    "    with torch.no_grad():\n",
    "        pred = predict_img(model, image)\n",
    "        pred = torch.max(pred, dim=0)[1]\n",
    "        miou, dice = metrics(pred, target).values()\n",
    "        avg_dice += dice\n",
    "        avg_mIoU += miou\n",
    "        \n",
    "avg_dice /= len(test_dataset)\n",
    "avg_mIoU /= len(test_dataset)\n",
    "print(f'Dice: {avg_dice:.2f}, mIoU: {avg_mIoU:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072b8c16-8815-4b4a-9635-dea0b0799dcf",
   "metadata": {},
   "source": [
    "**Задание**: напишите функцию инференса и прогоните модель на тестовом датасете. Также используйте метрики, чтобы узнать точность в таком полубоевом режиме. Как именно организовать инференс - ваше решение "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1762e5e7-ce19-4e36-b8ec-84bccbe88f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predicts(ds, model, n_ims):\n",
    "    plt.figure(figsize = (35, 30))\n",
    "    rows = n_ims // 3; cols = n_ims // rows\n",
    "    count = 1\n",
    "    indices = [random.randint(0, len(ds)) for _ in range(n_ims)]\n",
    "    for idx, index in enumerate(indices):\n",
    "        if count == n_ims + 1: break\n",
    "        im, gt = ds[index]\n",
    "        predicted_mask = predict_img(model, im)\n",
    "        predicted_mask = torch.max(predicted_mask, dim=0)[1]\n",
    "        # First Plot\n",
    "        count = plot(rows, cols, count, im = im * 255)\n",
    "        # Second Plot\n",
    "        count = plot(rows, cols, count, im = gt, gt = True, title=\"Segmentation mask\")\n",
    "        # Third Plot\n",
    "        miou, dice = metrics(predicted_mask, gt).values()\n",
    "        count = plot(rows, cols, count, im = predicted_mask, gt = True, title=f\"Predicted mask, mIoU:{miou:.2f}, Dice:{dice:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4fb223",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predicts(test_dataset, model, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6371b123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
