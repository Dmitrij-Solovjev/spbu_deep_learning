{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Transfer learning и ResNet\n",
    " В этот раз мы продолжим работать с изображениями с помощью нейронных сетей. В этот раз мы обсудим, как адаптировать уже существующие обученные модели для своих задач"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "План:\n",
    "- ResNet\n",
    "- transfer learning и fine-tuning\n",
    "- Визуализация модели\n",
    "- Torchmetrics, scheduler, ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchmetrics\n",
    "import matplotlib\n",
    "import tqdm\n",
    "from torch import nn\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Normalize, Resize, ToTensor\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"../../datasets/one-piece-classification/splitted/train/\"\n",
    "# TODO: change path to yours!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = ImageFolder(\n",
    "    DATA_PATH,\n",
    "    transform=Compose(\n",
    "        [\n",
    "            Resize((224, 224)),\n",
    "            ToTensor(),\n",
    "            Normalize((0.5, 0.5, 0.5), (1, 1, 1)),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "train_set, val_set = torch.utils.data.random_split(\n",
    "    dataset, [int(0.8 * len(dataset)), len(dataset) - int(0.8 * len(dataset))]\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_set, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "*Задание*: выведите случайный элемент датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Ace, Akainu, Brook, Chopper, Crocodile, Franky, Jinbei, Kurohige, Law, Luffy, Mihawk, Nami, Rayleigh, Robin, Sanji, Shanks, Usopp, Zoro\n",
    "\n",
    "folder= np.random.choice([\"Ace\", \"Akainu\", \"Brook\", \"Chopper\", \"Crocodile\", \"Franky\", \"Jinbei\", \"Kurohige\", \"Law\", \"Luffy\", \"Mihawk\", \"Nami\", \"Rayleigh\", \"Robin\", \"Sanji\", \"Shanks\", \"Usopp\", \"Zoro\"])\n",
    "file = np.random.choice(glob.glob(f'{DATA_PATH}/{folder}/*.*'))\n",
    "img = matplotlib.image.imread(file)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# выведите 352 семпл\n",
    "#plt.imshow(dataset[352][0].permute(1, 2, 0))\n",
    "\n",
    "plt.imshow(dataset[np.random.randint(0, 350)][0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Тут мы сталкиваемся с двумя проблемами сразу:\n",
    "- датасет очень маленький, и собирать его руками достаточно сложно\n",
    "- если мы попробуем обучить какую-то модель с нуля, это может занять очень много времени\n",
    "\n",
    "К тому же, наши картинки еще довольно похожи на стандартные изображения. Поэтому можно попытаться как-то перенести уже полученные знания какой-то крутой модели на свою задачу.\n",
    "Чаще всего для этого ипользуется файн-тюнинг.\n",
    "\n",
    "Идея у него проста: модель, когда-то обученная на большом датасете, уже умеет выделять какие-то хорошие признаки из входных данных, поэтому мы можем взять ее веса и немного поменять их, чтобы она лучше понимала новую задачу.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## ResNet\n",
    "Исследователи заметили, что когда речь идет о сверточных нейронных сетях, чем модель глубже, тем лучше. Это имеет смысл, потому что их гибкость в адаптации к любому пространству увеличивается с увеличением числа параметров). Однако было замечено, что начиная с некоторй глубины, точность перестает увеличиваться, а наоборот, начинает падать.\n",
    "*Вопрос:* с чем связано это явление?\n",
    "\n",
    "Так появилось семейство ResNet. Идея проста: еслт модель может предсказать $f(x)$, то может предсказать и $f(x) - x$.\n",
    "Это очень полезно, так как позволяет улучшить обучаемость моделей. (Чем?)\n",
    "![alt_text](../../additional_materials/images/resnet.png)\n",
    "\n",
    "В модели это реализовано следующим образом: к выходу слоя можно добавить его же вход (что распространяется и на более длинные связи).\n",
    "\n",
    "Обычный блок выглядит примерно следующим образом:\n",
    "\n",
    "Input: x\n",
    "\n",
    "[Conv 3x3]\n",
    "[Batch Normalization]\n",
    "[Activation (e.g., ReLU)]\n",
    "[Conv 3x3]\n",
    "[Batch Normalization]\n",
    "\n",
    "Add: x + F(x) (Output from the above layers)\n",
    "[Activation (e.g., ReLU)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Глубокие варианты используют не двухслойные блоки, но также и bottleneck блоки, которые состоят из трех сверточных слоев следующего вида:\n",
    "\n",
    "[Conv 1x1]\n",
    "[BN + ReLU]\n",
    "[Conv 3x3]\n",
    "[BN + ReLU]\n",
    "[Conv 1x1]\n",
    "[BN]\n",
    "\n",
    "*Вопрос*: Почему используются блоки 3x3?\n",
    "*Вопрос 2*: А почему по два таких слоя в блоке, а не один?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "В библиотеке torchvision имплементировано не только большое множество моделей (всевозможные ResNet'ы, Inception, VGG, AlexNet, DenseNet, ResNext, WideResNet, MobileNet...), но и загружены чекпоинты обучения этих моделей на ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# выведите модель\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Transfer learning\n",
    "В задаче transfer learning'a последний слой нейросети заменяется на линейный с нужным числом выходов, а веса остальных слоёв \"замораживаются\".\n",
    "Иногда делают полный файнтюн, тогда основные веса тоже обучаются, но в таком случае нужно задавать очень маленькие значения скорости обучения, чтобы не забыть то, что модель выучила раньше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Задание: определите, сколько классов в нашем датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "print(f'{len(dataset.class_to_idx)} classes')\n",
    "dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Теперь научимся загружать веса нашей модели, а также переделывать ее под свои нужды."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_model(\n",
    "        pretrained: bool = True, num_out_classes: int=10\n",
    "):\n",
    "    if pretrained:\n",
    "        model = resnet18(pretrained=True)\n",
    "        # freeze parameters of original model\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_out_classes)\n",
    "    else:\n",
    "        # <YOUR CODE>\n",
    "        model = resnet18(pretrained=False)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_out_classes)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Проверим нашу функцию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = get_model(pretrained=True, num_out_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Проверьте, задан ли градиент у какого-нибудь слоя из тела модели (это все, что не голова)\n",
    "next(model.layer1[0].conv1.parameters())[0].requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss() # задайте подходящий лосс"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "В этом ноутбуке мы рассмотрим еще очень полезную библиотеку, которая поможет нам считать метрики качества."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassPrecision, MulticlassRecall, MulticlassF1Score\n",
    "import torchmetrics.classification\n",
    "\n",
    "'''\n",
    "acc = torchmetrics.Accuracy(\n",
    "    task=\"multiclass\",\n",
    "    num_classes=18\n",
    ")\n",
    "\n",
    "F1score = torchmetrics.F1Score(\n",
    "    task=\"multiclass\",\n",
    "    num_classes=18\n",
    ")\n",
    "'''\n",
    "num_classes = 18\n",
    "metrics = MetricCollection({\n",
    "    \"acc\":    MulticlassAccuracy(num_classes),\n",
    "#    \"prec\":   MulticlassPrecision(num_classes),\n",
    "#    \"recall\": MulticlassRecall(num_classes),\n",
    "    \"F1\":     MulticlassF1Score(num_classes)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "*Задание*: добавьте метрики Precision, recall.\n",
    "*Задание 2*: сравните метрики с reduction=micro, macro. Ответьте, в чем разница и в каких ситуациях та или иная настройка лучше отражает качество предсказаний."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Вспомним рутину: напишем обучающий цикл. Заполните функциии в нужных местах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    metric: torchmetrics.MetricCollection,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    return_losses=False,\n",
    "):\n",
    "    model = model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    all_losses = []\n",
    "    with tqdm.tqdm(total=len(data_loader)) as prbar:\n",
    "        for inputs, targets in data_loader:\n",
    "            # train step: preds, loss, optimization, etc\n",
    "            # <YOUR CODE>\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(inputs)\n",
    "            loss = criterion(preds, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            # update description for tqdm\n",
    "            metrics_dict = metric(preds, targets)\n",
    "            metrics_str = f\"Loss: {round(loss.item(), 4)} \"\n",
    "            for k, v in metrics_dict.items():\n",
    "                metrics_str += f\"{k}: {round(float(v), 4)} \"\n",
    "\n",
    "            prbar.set_description(\n",
    "                metrics_str\n",
    "            )\n",
    "            prbar.update(1)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            all_losses.append(loss.detach().item())\n",
    "    total_metrics_dict = metric.compute()\n",
    "    #metrics = {\"train_loss\": total_loss / num_batches, \"train_accuracy\": metrics_dict}\n",
    "    total_metrics_dict[\"train_loss\"] = total_loss\n",
    "    if return_losses:\n",
    "        return total_metrics_dict, all_losses\n",
    "    else:\n",
    "        return total_metrics_dict\n",
    "\n",
    "\n",
    "def validate(\n",
    "        model,\n",
    "        data_loader,\n",
    "        criterion,\n",
    "        metric: torchmetrics.MetricCollection,\n",
    "        device=\"cuda:0\"):\n",
    "    model = model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    with tqdm.tqdm(total=len(data_loader)) as prbar:\n",
    "        for inputs, targets in data_loader:\n",
    "            with torch.no_grad():\n",
    "                # <YOUR CODE>\n",
    "                preds = model(inputs)\n",
    "                loss = criterion(preds, targets)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                metrics_dict = metric(preds, targets)\n",
    "            \n",
    "            metrics_str = f\"Loss: {round(loss.item(), 4)} \"\n",
    "            for k, v in metrics_dict.items():\n",
    "                metrics_str += f\"{k}: {round(float(v), 4)} \"\n",
    "            prbar.set_description(metrics_str)\n",
    "            prbar.update(1)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    total_metrics_dict = metric.compute()\n",
    "    #metrics = {\"val_loss\": total_loss / num_batches} # <YOUR CODE> total_metrics_dict\n",
    "    total_metrics_dict[\"val_loss\"] = total_loss\n",
    "    return total_metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Иногда удобно сохранять данные в определенном формате. Один из возможных вариантов - namedtuple. Он похволяет как сохранять порядок элементов, так и использовать более понятную индексацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "LossInfo = namedtuple(\n",
    "    \"LossInfo\", [\"full_train_losses\", \"train_epoch_losses\",\n",
    "                    \"epoch_train_acc_losses\", \"epoch_train_f1_losses\",\n",
    "                 \"epoch_val_loss_losses\", \"epoch_val_acc_losses\", \"epoch_val_f1_losses\"]\n",
    ")\n",
    "\n",
    "EPOCHS = 10\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    epochs,\n",
    "    train_data_loader,\n",
    "    validation_data_loader,\n",
    "    criterion,\n",
    "    metrics,\n",
    "    optimizer,\n",
    "    scheduler=None,\n",
    "    device=\"cuda:0\",\n",
    "):\n",
    "    all_train_losses = []\n",
    "    epoch_train_losses = []\n",
    "    epoch_train_acc_losses = []\n",
    "    epoch_train_f1_losses = []\n",
    "\n",
    "    epoch_val_acc_losses = []\n",
    "    epoch_val_f1_losses = []\n",
    "    epoch_val_loss_losses = []\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # construct iterators\n",
    "        train_iterator = iter(train_data_loader)\n",
    "        validation_iterator = iter(validation_data_loader)\n",
    "        # train step\n",
    "        print(f\"Train Epoch: {epoch}\")\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        train_metrics_collection = metrics.clone(prefix='t_')\n",
    "        valid_metrics_collection = metrics.clone(prefix='v_')\n",
    "\n",
    "        train_metrics, one_epoch_train_losses = train_epoch(\n",
    "            model=model,\n",
    "            data_loader=train_iterator,\n",
    "            criterion=criterion,\n",
    "            metric=train_metrics_collection,\n",
    "            optimizer=optimizer,\n",
    "            return_losses=True,\n",
    "        )\n",
    "\n",
    "        # save train losses\n",
    "        all_train_losses.append(one_epoch_train_losses)\n",
    "        epoch_train_losses.append(train_metrics[\"train_loss\"])\n",
    "        epoch_train_acc_losses.append(train_metrics[\"t_acc\"])\n",
    "        epoch_train_f1_losses.append(train_metrics[\"t_F1\"])\n",
    "\n",
    "        # eval step\n",
    "        # call function validation epoch and\n",
    "        # save eval losses\n",
    "        model.eval()\n",
    "        print(f\"Validation Epoch: {epoch}\")\n",
    "\n",
    "        test_metrics = validate(\n",
    "            model=model,\n",
    "            data_loader=validation_iterator,\n",
    "            criterion=criterion,\n",
    "            metric=valid_metrics_collection,\n",
    "        )\n",
    "\n",
    "        print(test_metrics)\n",
    "\n",
    "        epoch_val_acc_losses.append(test_metrics[\"v_acc\"])\n",
    "        epoch_val_f1_losses.append(test_metrics[\"v_F1\"])\n",
    "        epoch_val_loss_losses.append(test_metrics[\"val_loss\"])\n",
    "\n",
    "\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "    return LossInfo(all_train_losses, epoch_train_losses, epoch_train_acc_losses, epoch_train_f1_losses,\n",
    "                    epoch_val_loss_losses, epoch_val_acc_losses, epoch_val_f1_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "*Задание*: запустите обучение с использованием transfer learning и без него. В каком случае точность получилась выше?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(pretrained=True, num_out_classes=18).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR) # задайте оптимизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "_ = train(\n",
    "    model=model,\n",
    "    epochs=10,\n",
    "    train_data_loader=train_dataloader,\n",
    "    validation_data_loader=val_dataloader,\n",
    "    criterion=loss,\n",
    "    metrics=metrics,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asodoasod' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43masodoasod\u001b[49m \n",
      "\u001b[0;31mNameError\u001b[0m: name 'asodoasod' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# You should not pass)\n",
    "asodoasod "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Визуализация сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Каждый слой в CNN изучает фильтры возрастающей сложности. На первых слоях выучиваются простые признаки, такие как края и углы. Средние слои обнаруживают части объектов — что касается лиц, они могут научиться реагировать на глаза и носы. Последние слои имеют более высокоуровневые представления: они учатся распознавать объекты целиком, в различных формах и положениях.\n",
    "\n",
    "Мы попробуем проверить, так ли это, с помощью GradientExplainer, который использует ожидаемые градиенты для оценки входов в разные части модели. В целом, они аппроксимируют значения SHAP.\n",
    "\n",
    "Возьмем по 50 семплов для подсчёта ожидаемых градиентов и посмотрим на признаки первых слоёв ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "to_explain = torch.stack([dataset[0][0], dataset[123][0]], dim=0)\n",
    "e = shap.GradientExplainer((model, model.layer1[0].conv2), to_explain)\n",
    "# receive shap_values\n",
    "shap_values, indexes = e.shap_values(\n",
    "    to_explain, ranked_outputs=4, nsamples=50\n",
    ")\n",
    "\n",
    "# plot the explanations\n",
    "#shap_values = [np.swapaxes(np.swapaxes(s, 2, 3), 1, -1) for s in shap_values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shap_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shap.image_plot(shap_values[0], to_explain.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shap.image_plot(shap_values[1], to_explain.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Повторите для второго слоя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "to_explain = torch.stack([dataset[0][0], dataset[123][0]], dim=0).to(device)\n",
    "e = shap.GradientExplainer((model, model.layer2[0].conv2), to_explain)\n",
    "# receive shap_values\n",
    "shap_values, indexes = e.shap_values(\n",
    "    to_explain, ranked_outputs=4, nsamples=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "shap.image_plot(shap_values[0], to_explain.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Повторите для 4 слоя)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "to_explain = torch.stack([dataset[0][0], dataset[123][0]], dim=0).to(device)\n",
    "e = shap.GradientExplainer((model, model.layer4[0].conv2), to_explain)\n",
    "# receive shap_values\n",
    "shap_values, indexes = e.shap_values(\n",
    "    to_explain, ranked_outputs=4, nsamples=50\n",
    ")\n",
    "shap.image_plot(shap_values[0], to_explain.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Но это не единственный способ заглянуть внутрь модели. Простейший вариант - оценить значения активаций на внутренних слоях. Можно было бы вывести веса, но они, к сожалению, не слишком интерпретируемы. Активации же показывают силу реакции конкретного фильтра."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Далее попробуем посмотреть на значения активаций при при проходе через изображение. В этом нам помогут хуки - эти функции позволяют получить доступ к состоянию между слоями (как входа, так и выхода слоя)\n",
    "для визуализации, трансфер-лернинга, дебага модели итд."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "activations = {}\n",
    "\n",
    "# Function to create a hook\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# For example, let's hook into the first convolutional layer and the first block of layer1\n",
    "model.conv1.register_forward_hook(get_activation('conv1'))\n",
    "model.layer1[0].register_forward_hook(get_activation('layer1_block1'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to visualize feature maps\n",
    "def visualize_feature_maps(activation, title, num_cols=8):\n",
    "    num_feature_maps = activation.shape[0]\n",
    "    num_rows = num_feature_maps // num_cols\n",
    "    plt.figure(figsize=(num_cols, num_rows))\n",
    "    for i in range(num_feature_maps):\n",
    "\n",
    "        if i >= num_cols * num_rows:\n",
    "            break\n",
    "        plt.subplot(num_rows, num_cols, i+1)\n",
    "        plt.imshow(activation[i].cpu().numpy(), cmap='coolwarm')\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Выведите значения активаций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform a forward pass\n",
    "with torch.no_grad():\n",
    "    # прогоните модель для первого элемента датасета\n",
    "    output = model(dataset[0][0].unsqueeze(0).to(device)) #your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name, activation in activations.items():\n",
    "    # For convolutional layers, activation shape is (batch_size, num_channels, H, W)\n",
    "    act = activation[0]\n",
    "    act = act - act.min() # Почему нужно нормализовать?\n",
    "    act = act / act.max()\n",
    "    visualize_feature_maps(act, title=f'Activations from {name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "попробуйте добавить хук и для 4 слоя. Какие значения у активаций? Похоже ли это на то, что мы видели для shap values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "activations = {}\n",
    "\n",
    "# Function to create a hook\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# For example, let's hook into the first convolutional layer and the first block of layer1\n",
    "model.conv1.register_forward_hook(get_activation('conv1'))\n",
    "model.layer4[0].register_forward_hook(get_activation('layer_block1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a forward pass\n",
    "with torch.no_grad():\n",
    "    # прогоните модель для первого элемента датасета\n",
    "    output = model(dataset[0][0].unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, activation in activations.items():\n",
    "    # For convolutional layers, activation shape is (batch_size, num_channels, H, W)\n",
    "    act = activation[0]\n",
    "    act = act - act.min() # Почему нужно нормализовать?\n",
    "    act = act / act.max()\n",
    "    visualize_feature_maps(act, title=f'Activations from {name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
