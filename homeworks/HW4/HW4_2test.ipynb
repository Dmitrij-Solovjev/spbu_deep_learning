{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Iterable, Tuple\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.distributions.categorical import Categorical\n",
    "import tqdm\n",
    "import re\n",
    "from functools import cmp_to_key\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "with open(\"../../datasets/anek_djvu.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|startoftext|>Друзья мои, чтобы соответствовать вам, я готов сделать над собой усилие и стать лучше. Но тогда и вы станьте немного хуже!\\n\\n<|startoftext|>- Люся, ты все еще хранишь мой подарок?- Да.- Я думал, ты выкинула все, что со мной связано.- Плюшевый мишка не виноват, что ты ебл@н...\\n\\n<|startoftext|>- А вот скажи честно, ты во сне храпишь?- Понятие не имею, вроде, нет. От со'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[118:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбитие на анектоды\n",
    "def cut_data(text):\n",
    "    return text.replace(\"\\n\\n\", \"\").split(\"<|startoftext|>\")[1:]\n",
    "cut_text = cut_data(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Лайфхак. Положив сахар в чай, не размешивайте его до тех пор, пока не дойдете до своего компьютера. Та лужа, которую вы сделаете по пути, будет нелипкая и несладкая, и вы сможете спокойно вытереть ее своим носком.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "cut_text[random.randint(0, 31000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_symbols = {'\\n', ':', ';', '=', '>', '?', '@', ' ', '!', '\"', '#', '$', '%', '&', '\\'', '*', '+', ',', '\\-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Ё', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Ъ', 'Ы', 'Ь', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё'}\n",
    "\n",
    "allowed_symbols2 = {'\\n', ':', ';', '=', '>', '?', '@', ' ', '!', '\"', '#', '$', '%', '&', '\\'', '*', '+', ',', '\\-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Ё', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Ъ', 'Ы', 'Ь', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё'}\n",
    "\n",
    "allowed_symbols2_rus = {':', ';', '?', '@', ' ', '!', '\"', '#', '$', '%', '*', '+', ',', '\\-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9','Ё', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Ъ', 'Ы', 'Ь', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_MAX_TOKEN_LEN = 2\n",
    "GLOBAL_MAX_TOKEN_COUNT = 61"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 4. RNN\n",
    "## Задача 1. 3 балла\n",
    "Обучите RNN/LSTM на данных из классной работы, используя другой токенайзер. Опишите его и свой выбор. Покажите разницу в генерации моделей, обученных с разными токенайзерами.\n",
    "## {*} Задача 1.1 2 балла\n",
    "Напишите свой токенайзер вручную, с использованием только библиотек numpy, torch, sklearn, stats, опционально других пакетов, не предоставляющих готовые инструменты токенизации и т.п., за исключением предобработки текста (лемматизация, стеминг и т.д.) . \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Tokenizer:\n",
    "    def __init__(self, cut_text, max_count_token: int = 512, max_token_len = 2, allowed_s = allowed_symbols2_rus):\n",
    "        self.allowed_s = allowed_s\n",
    "        self.max_token_len = max_token_len\n",
    "        self.text = cut_text\n",
    "        self.max_count_token = max_count_token\n",
    "        self.specials = {'<pad>':999999, '<bos>':999999, '<eos>':999999}\n",
    "        \n",
    "        self.create_vocab(\"\".join(self.text))\n",
    "        \n",
    "    def token_compare(self, item1:dict, item2:dict):\n",
    "        if ((\" \" in item1[0] or \" \" in item2[0]) and not(item1[0] == \" \" or item2[0] == \" \")):\n",
    "            #print(item1[0], item2[0])\n",
    "            if (\" \" in item1[0] and \" \" in item2[0]):\n",
    "                return 0\n",
    "            elif (\" \" in item1[0]):\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        else:\n",
    "            #if (len(item2[0]) > len(item1[0]) and item2[0])\n",
    "            return item2[1] - item1[1]\n",
    "    \n",
    "    def create_vocab(self, text: str):\n",
    "        regex = r'[^' + \"\".join(self.allowed_s).lower() + ']'\n",
    "        reg = re.compile(regex)\n",
    "        text = reg.sub('', text.replace('ё', 'е'))\n",
    "    \n",
    "        my_dict = {}\n",
    "        i = 1\n",
    "        text_len = len(text)\n",
    "        for index in range(0, text_len - i + 1):\n",
    "            token = text[index:index+i]\n",
    "            if not token in my_dict:\n",
    "                my_dict[token]=9999999\n",
    "    \n",
    "        text = re.sub(r'[^а-я ]', '', text)\n",
    "\n",
    "    \n",
    "        for i in range (2, self.max_token_len+1):\n",
    "            text_len = len(text)\n",
    "            for index in range(0, text_len - i + 1):\n",
    "                token = text[index:index+i]\n",
    "                if token in my_dict:\n",
    "                    my_dict[token]+=1\n",
    "                else:\n",
    "                    my_dict[token]=1\n",
    "                # Assign unique IDs to tokens\n",
    "        \n",
    "        # отсортировали по принципу с пробелом в конец, 1 символ в начало,\n",
    "        #   остальное по частоте, чтобы лего сделать crop\n",
    "        sorted_tokens = dict(sorted(my_dict.items(), key=cmp_to_key(self.token_compare))[:self.max_count_token])\n",
    "        # Сортируем по длине токена, а после по частоте\n",
    "        sorted_tokens.update(self.specials)\n",
    "\n",
    "        sorted_tokens = sorted(sorted_tokens.items(), key=lambda x: (-len(x[0]), -x[1]))\n",
    "\n",
    "        self.token_to_id = {token: idx for idx, (token, _) in enumerate(sorted_tokens)}\n",
    "        self.id_to_token = {idx: token for token, idx in self.token_to_id.items()}\n",
    "    \n",
    "    def _add_special(self, symbol) -> None:\n",
    "        # add special characters to yuor dicts\n",
    "        sym_num = len(self.token_to_id)\n",
    "        self.token_to_id[symbol] = sym_num\n",
    "        self.id_to_token[sym_num] = symbol\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.token_to_id) # your code\n",
    "    \n",
    "    def decode_symbol(self, el):\n",
    "        return self.id_to_token[el]\n",
    "        \n",
    "    def encode_symbol(self, el):\n",
    "        return self.token_to_id[el]\n",
    "\n",
    "    #@property\n",
    "    def tokenize(self, text: str):\n",
    "        \"\"\"\n",
    "        Tokenizes the input text into numbers using the built dictionary.\n",
    "        :param text: Input text to tokenize.\n",
    "        :return: A list of token IDs representing the text.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.token_to_id:\n",
    "            raise ValueError(\"Tokenizer dictionary is empty. Call 'build_dict' first.\")\n",
    "        \n",
    "        tokens = []\n",
    "        i = 0\n",
    "        while i < len(text):\n",
    "            match = None\n",
    "            # Try to find the longest matching token\n",
    "            for size in range(max(self.max_token_len, 5), 0, -1):\n",
    "                if i + size <= len(text):\n",
    "                    substring = text[i:i + size]\n",
    "                    if substring in self.token_to_id:\n",
    "                        match = substring\n",
    "                        break\n",
    "            \n",
    "            if match:\n",
    "                tokens.append(self.token_to_id[match])\n",
    "                i += len(match)  # Move past the matched token\n",
    "            else:\n",
    "                # If no match, handle single characters (fallback)\n",
    "                tokens.append(self.token_to_id[text[i]])\n",
    "                i += 1\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def detokenize(self, token_ids):\n",
    "        \"\"\"\n",
    "        Converts token IDs back into the original text using the dictionary.\n",
    "        :param token_ids: A list of token IDs.\n",
    "        :return: The reconstructed text.\n",
    "        \"\"\"\n",
    "        return ''.join(self.id_to_token[token_id] for token_id in token_ids)\n",
    "    \n",
    "    def encode(self, chars, eos=True):\n",
    "        regex = r'[^' + \"\".join(self.allowed_s) + ']'\n",
    "        reg = re.compile(regex)\n",
    "        chars = reg.sub('', chars.lower().replace('ё', 'е'))\n",
    "\n",
    "        if eos:\n",
    "            chars = '<bos>' + chars + '<eos>'\n",
    "        else:\n",
    "            chars = '<bos>' + chars\n",
    "        return self.tokenize(chars)\n",
    "\n",
    "    def decode(self, idx):\n",
    "        return self.detokenize(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JokesDataset(Dataset):\n",
    "    def __init__(self, tokenizer, cut_text, max_len: int = 512):\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cut_text = cut_text\n",
    "        self.pad_index = self.tokenizer.encode_symbol(\"<pad>\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cut_text)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        #  в идеале запонлять паддингами лучше в другом месте\n",
    "        encoded = self.tokenizer.encode(self.cut_text[item])[:self.max_len]\n",
    "        padded = torch.full((self.max_len, ), self.pad_index, dtype=torch.long)\n",
    "        padded[:len(encoded)] = torch.tensor(encoded)\n",
    "        # pad your sequence and make a final sample. You can skip padding and pad sequences with torch special method.\n",
    "        return padded, len(encoded)\n",
    "\n",
    "# Optionally add new methods to your dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 2. 3 балла\n",
    "Реализуйте с помощью только torch/numpy слой RNN, обучите его на данных из классной работы и, опционально, своих данных. Покажите, что модель обучается\n",
    "## {*} Задача 2.1 +1 балл\n",
    "За реализацию слоев GRU/LSTM/bidirectional RNN, многослойной модели по +1 баллу к базовым (даже если ванильная RNN не реализована)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\n",
    "```python\n",
    "self.rnn = nn.RNN(\n",
    "            input_size=self.hidden_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вопроc: зачем num_layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRNN(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            hidden_size:int,\n",
    "            seq_len:int):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_len = seq_len\n",
    "        self.hidden_size = hidden_size        \n",
    "        \n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        \n",
    "\n",
    "        # Кратко поясню: сходимости не было, был либо разброс,\n",
    "        #   либо затухание, поэтому nn\n",
    "\n",
    "        self.Lh = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Lx = nn.Linear(hidden_size, hidden_size)\n",
    "        #self.Wh = torch.rand(hidden_size, hidden_size, dtype=torch.float32, requires_grad=True)\n",
    "        #self.Wx = torch.rand(hidden_size, hidden_size, dtype=torch.float32, requires_grad=True)\n",
    "        self.Lh = torch.nn.utils.spectral_norm(self.Lh)\n",
    "        self.Lx = torch.nn.utils.spectral_norm(self.Lx)\n",
    "\n",
    "        self.bh = torch.rand((1, hidden_size), dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros((1, self.hidden_size), dtype=torch.float32)\n",
    "        \n",
    "        h_t_minus_1 = h_0\n",
    "        h_t = h_0\n",
    "        output = []\n",
    "        print(len(x[0]))\n",
    "        for t in range(len(x[0])):\n",
    "            #h_t = torch.tanh(\n",
    "            #    x[t] @ self.Wx.T\n",
    "            #    + h_t_minus_1 @ self.Wh.T\n",
    "            #    + self.bh\n",
    "            #)\n",
    "            h_t = self.Lh(h_t_minus_1) + self.Lx(x[0][t]) + self.bh\n",
    "            output.append(h_t)\n",
    "            h_t_minus_1 = h_t\n",
    "        \n",
    "        output = torch.stack(output)\n",
    "        output = output.transpose(0, 1)\n",
    "        return output, None #h_t.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        hidden_dim: int = 256,\n",
    "        num_layers: int = 2,\n",
    "        drop_prob: float = 0.5,\n",
    "        max_text_len: int = 512, # Максимальная длина сообщения в символах\n",
    "    ) -> None:\n",
    "        '''    hidden_dim: Длинна словаря (количество ожидаемых признаков во входных данных x)\n",
    "               max_len: Максимальная длина сообщения в символах\n",
    "        '''\n",
    "        \n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.max_text_len = max_text_len\n",
    "        # create mappings\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        ## define the LSTM, dropout and fully connected layers\n",
    "        self.encoder = nn.Embedding(self.tokenizer.vocab_size, self.hidden_dim)\n",
    "        self.rnn = CustomRNN(\n",
    "            hidden_size=self.hidden_dim,\n",
    "            seq_len=4\n",
    "        )\n",
    "        #self.rnn = nn.LSTM(\n",
    "        #    input_size=self.hidden_dim,\n",
    "        #    hidden_size=self.hidden_dim,\n",
    "        #    num_layers=1,\n",
    "        #    batch_first=True,\n",
    "        #    dropout=self.drop_prob\n",
    "        #)\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(p=self.drop_prob)\n",
    "        self.decoder = nn.Linear(\n",
    "            in_features=self.hidden_dim,\n",
    "            out_features=self.tokenizer.vocab_size,\n",
    "        )\n",
    "\n",
    "    # Forward - это проход вперёд по слою\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, lengths: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # one-hot encode your sequence\n",
    "        packed_embeds = self.encoder(x) # pack your sequence. This helps with the efficiency. Use torch function pack_padded_sequence\n",
    "        outputs, hidden = self.rnn(packed_embeds) # run you model\n",
    "        #print(outputs)\n",
    "        # TODO: Понять нафига\n",
    "        #  out, lengths = # pad sequence back\n",
    "        \n",
    "        # Pass through a dropout layer and fully connected layer\n",
    "        out = self.dropout(outputs)\n",
    "        ## Get the output for classification.\n",
    "        out = self.decoder(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    \n",
    "    # инференс - режим не обучения (По сути штатная работа)\n",
    "    def inference(self, prefix='<bos> ', device=\"cpu\"):\n",
    "        tokens = torch.tensor([self.tokenizer.encode(prefix, eos=False)], device=device) # encode prefix\n",
    "        \n",
    "        # 2 stopping conditions: reaching max len or getting <eos> token\n",
    "        # Generate sequence iteratively\n",
    "        for _ in range(self.max_text_len - len(tokens[0])):\n",
    "            # YOUR CODE: generate sequence one by one\n",
    "            # Pass tokens through the embedding layer\n",
    "            logits, hidden = self.forward(tokens, torch.tensor([tokens.size(1)]))\n",
    "            \n",
    "            # Get the last token's logits and sample a token\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            new_token = torch.multinomial(\n",
    "                torch.nn.functional.softmax(next_token_logits, dim=-1), num_samples=1\n",
    "            )\n",
    "\n",
    "            # Append the new token\n",
    "            tokens = torch.cat([tokens, new_token], dim=1)\n",
    "\n",
    "            # Stop if the <eos> token is generated\n",
    "            if new_token.item() == self.tokenizer.encode_symbol(\"<eos>\"):\n",
    "                break\n",
    "        # Decode the token IDs back into a string\n",
    "        return self.tokenizer.decode(tokens.squeeze().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(\n",
    "    model: CharRNN,\n",
    "    train_batch: Tuple[torch.Tensor, torch.Tensor],\n",
    "    vocab_size: int,\n",
    "    criterion: nn.Module,\n",
    "    optimizer,\n",
    "    device=\"cpu\"\n",
    ") -> torch.Tensor:\n",
    "    inputs, lengths = train_batch\n",
    "    inputs = inputs.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "\n",
    "    # Сброс градиентов\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Прямой проход\n",
    "    outputs, _ = model(inputs[:, :-1], lengths)\n",
    "\n",
    "    # Переформатирование выходов и целевых меток для расчета функции потерь\n",
    "    outputs = outputs.view(-1, vocab_size)\n",
    "    targets = inputs[:, 1:].reshape(-1)\n",
    "\n",
    "    # Вычисление функции потерь\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Обратный проход\n",
    "    loss.backward()\n",
    "\n",
    "    # Шаг оптимизации\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(losses):\n",
    "    clear_output()\n",
    "    plt.plot(range(1, len(losses) + 1), losses)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "n_hidden = 512 # 10+3 #64 #256\n",
    "n_layers = 1 #4\n",
    "drop_prob = 0.1\n",
    "lr = 0.001\n",
    "\n",
    "num_epochs = 1\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Custom_Tokenizer(cut_text,\n",
    "                             max_count_token=GLOBAL_MAX_TOKEN_COUNT,\n",
    "                             max_token_len = GLOBAL_MAX_TOKEN_LEN,\n",
    "                             allowed_s = allowed_symbols2_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRNN(tokenizer, hidden_dim=n_hidden, num_layers=n_layers, drop_prob=drop_prob)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = JokesDataset(tokenizer, cut_text, 256)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 1, 13, 33, 38, 16,  9, 33, 10, 22,  6,  7, 14, 11, 25,  6, 10, 11,  4,\n",
       "          15,  8, 10, 39, 10, 19, 33, 13, 14, 20, 10, 24, 12, 11, 32, 14, 15,  8,\n",
       "          10, 38, 14, 25, 41, 16, 25, 33, 10, 11, 12, 10, 22, 27, 15,  8, 10, 13,\n",
       "          16, 25, 33, 15, 17, 10, 22, 24, 16, 32, 14, 15, 19, 27, 10,  6, 32, 14,\n",
       "          20, 12, 15,  8, 10, 14, 14, 10, 20, 19, 40, 10, 38, 16, 11, 25,  8, 31,\n",
       "          31, 31,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0]]),\n",
       " tensor([93])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/124155 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 4.4596 :   0%|          | 100/124155 [00:00<06:49, 302.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 4.4596 :   0%|          | 100/124155 [00:19<06:49, 302.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5782 :   0%|          | 200/124155 [00:25<5:08:01,  6.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5782 :   0%|          | 200/124155 [00:34<5:53:56,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)) \u001b[38;5;28;01mas\u001b[39;00m prbar:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m----> 7\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m         epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;66;03m#print(f'Done {i/len(dataloader) * 100:.2f}%, Loss: {loss:.4f}')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m, in \u001b[0;36mtraining_step\u001b[0;34m(model, train_batch, vocab_size, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Прямой проход\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m outputs, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Переформатирование выходов и целевых меток для расчета функции потерь\u001b[39;00m\n\u001b[1;32m     20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size)\n",
      "File \u001b[0;32m~/Документы/Першин_Никольская_Нейронные_сети/.myvenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Документы/Першин_Никольская_Нейронные_сети/.myvenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 50\u001b[0m, in \u001b[0;36mCharRNN.forward\u001b[0;34m(self, x, lengths)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, lengths: torch\u001b[38;5;241m.\u001b[39mTensor\n\u001b[1;32m     47\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# one-hot encode your sequence\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     packed_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x) \u001b[38;5;66;03m# pack your sequence. This helps with the efficiency. Use torch function pack_padded_sequence\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     outputs, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked_embeds\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# run you model\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m#print(outputs)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# TODO: Понять нафига\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m#  out, lengths = # pad sequence back\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# Pass through a dropout layer and fully connected layer\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(outputs)\n",
      "File \u001b[0;32m~/Документы/Першин_Никольская_Нейронные_сети/.myvenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Документы/Першин_Никольская_Нейронные_сети/.myvenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 40\u001b[0m, in \u001b[0;36mCustomRNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m255\u001b[39m):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m#h_t = torch.tanh(\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m#    x[t] @ self.Wx.T\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m#    + h_t_minus_1 @ self.Wh.T\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#    + self.bh\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m#)\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     h_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_t_minus_1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLx(x[\u001b[38;5;241m0\u001b[39m][t]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbh\n\u001b[1;32m     41\u001b[0m     output\u001b[38;5;241m.\u001b[39mappend(h_t)\n\u001b[1;32m     42\u001b[0m     h_t_minus_1 \u001b[38;5;241m=\u001b[39m h_t\n",
      "File \u001b[0;32m~/Документы/Першин_Никольская_Нейронные_сети/.myvenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Документы/Першин_Никольская_Нейронные_сети/.myvenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1592\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1587\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1588\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward pre-hook must return None or a tuple \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1589\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs_kwargs_result\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1590\u001b[0m             )\n\u001b[1;32m   1591\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1592\u001b[0m     args_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1593\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1594\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args_result, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/Документы/Першин_Никольская_Нейронные_сети/.myvenv/lib/python3.12/site-packages/torch/nn/utils/spectral_norm.py:106\u001b[0m, in \u001b[0;36mSpectralNorm.__call__\u001b[0;34m(self, module, inputs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, module: Module, inputs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(module, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_power_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Документы/Першин_Никольская_Нейронные_сети/.myvenv/lib/python3.12/site-packages/torch/nn/utils/spectral_norm.py:77\u001b[0m, in \u001b[0;36mSpectralNorm.compute_weight\u001b[0;34m(self, module, do_power_iteration)\u001b[0m\n\u001b[1;32m     75\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_u\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     76\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_v\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m weight_mat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape_weight_to_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_power_iteration:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/Документы/Першин_Никольская_Нейронные_сети/.myvenv/lib/python3.12/site-packages/torch/nn/utils/spectral_norm.py:35\u001b[0m, in \u001b[0;36mSpectralNorm.reshape_weight_to_matrix\u001b[0;34m(self, weight)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_power_iterations \u001b[38;5;241m=\u001b[39m n_power_iterations\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps \u001b[38;5;241m=\u001b[39m eps\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreshape_weight_to_matrix\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     36\u001b[0m     weight_mat \u001b[38;5;241m=\u001b[39m weight\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;66;03m# permute dim to front\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_loss = 0\n",
    "    with tqdm.tqdm(total=len(dataloader)) as prbar:\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            loss = training_step(model, batch, tokenizer.vocab_size, criterion, optimizer, device)\n",
    "            epoch_loss += loss\n",
    "        \n",
    "            if i % 100 == 0:\n",
    "                #print(f'Done {i/len(dataloader) * 100:.2f}%, Loss: {loss:.4f}')\n",
    "                metrics_str = f\"Loss: {round(loss, 4)} \"\n",
    "                #for k, v in metrics_dict.items():\n",
    "                #    metrics_str += f\"{k}: {round(float(v), 4)} \"\n",
    "                prbar.set_description(metrics_str)\n",
    "                prbar.update(100)\n",
    "    \n",
    "    epoch_loss /= len(dataloader)\n",
    "    losses.append(epoch_loss)\n",
    "    \n",
    "    plot_losses(losses)\n",
    "    #torch.save(model.state_dict(), \"rnn.pt\")\n",
    "    torch.save(model, \"rnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(\"rnn.pt\", weights_only=True))\n",
    "model = torch.load(\"rnn.pt\", weights_only=False)\n",
    "\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 27 is out of bounds for dimension 0 with size 27",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m [\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mПилите, Шура, пилите. Они \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)]\n",
      "Cell \u001b[0;32mIn[10], line 72\u001b[0m, in \u001b[0;36mCharRNN.inference\u001b[0;34m(self, prefix, device)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# 2 stopping conditions: reaching max len or getting <eos> token\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Generate sequence iteratively\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_text_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens[\u001b[38;5;241m0\u001b[39m])):\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# YOUR CODE: generate sequence one by one\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# Pass tokens through the embedding layer\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m     logits, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# Get the last token's logits and sample a token\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     next_token_logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "Cell \u001b[0;32mIn[10], line 50\u001b[0m, in \u001b[0;36mCharRNN.forward\u001b[0;34m(self, x, lengths)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, lengths: torch\u001b[38;5;241m.\u001b[39mTensor\n\u001b[1;32m     47\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# one-hot encode your sequence\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     packed_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x) \u001b[38;5;66;03m# pack your sequence. This helps with the efficiency. Use torch function pack_padded_sequence\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     outputs, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked_embeds\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# run you model\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m#print(outputs)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# TODO: Понять нафига\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m#  out, lengths = # pad sequence back\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# Pass through a dropout layer and fully connected layer\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(outputs)\n",
      "File \u001b[0;32m~/Документы/Першин_Никольская_Нейронные_сети/.myvenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Документы/Першин_Никольская_Нейронные_сети/.myvenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 40\u001b[0m, in \u001b[0;36mCustomRNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m255\u001b[39m):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m#h_t = torch.tanh(\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m#    x[t] @ self.Wx.T\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m#    + h_t_minus_1 @ self.Wh.T\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#    + self.bh\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m#)\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     h_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLh(h_t_minus_1) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLx(\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbh\n\u001b[1;32m     41\u001b[0m     output\u001b[38;5;241m.\u001b[39mappend(h_t)\n\u001b[1;32m     42\u001b[0m     h_t_minus_1 \u001b[38;5;241m=\u001b[39m h_t\n",
      "\u001b[0;31mIndexError\u001b[0m: index 27 is out of bounds for dimension 0 with size 27"
     ]
    }
   ],
   "source": [
    "[model.inference(\"Пилите, Шура, пилите. Они \", device=device) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 3. 1/2/3/4 балла\n",
    "**TBD**: \n",
    "Попробуйте обучить рекуррентную сеть задаче классификации. Вы можете воспользоваться сторонними библиотеками для вашей работы, \n",
    "но модель и основной код должны быть написаны на pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  {*} Задача 4. 5/6/7/8 баллов\n",
    "[ссылка](https://www.kaggle.com/t/b2ef08dc3ddf44f981e2ad186c6c508d)\n",
    "\n",
    "Попробуйте обучить сверточную нейронную сеть задаче детекции людей на изображениях разного стиля. Вы можете воспользоваться сторонними библиотеками для вашей работы. Однако, за неисопользование полностью готовых скриптов обучения (как в классной работе) вы получите дополнительные2 балла"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
