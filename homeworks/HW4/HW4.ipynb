{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Iterable, Tuple\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.distributions.categorical import Categorical\n",
    "import tqdm\n",
    "import re\n",
    "from functools import cmp_to_key\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "with open(\"../../datasets/anek_djvu.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|startoftext|>Друзья мои, чтобы соответствовать вам, я готов сделать над собой усилие и стать лучше. Но тогда и вы станьте немного хуже!\\n\\n<|startoftext|>- Люся, ты все еще хранишь мой подарок?- Да.- Я думал, ты выкинула все, что со мной связано.- Плюшевый мишка не виноват, что ты ебл@н...\\n\\n<|startoftext|>- А вот скажи честно, ты во сне храпишь?- Понятие не имею, вроде, нет. От со'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[118:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбитие на анектоды\n",
    "def cut_data(text):\n",
    "    return text.replace(\"\\n\\n\", \"\").split(\"<|startoftext|>\")[1:]\n",
    "cut_text = cut_data(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Жизнь в России стала гораздо лучше: если раньше из нее бежали обиженные и ограбленные, то теперь ее покидают разбогатевшие и довольные.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "cut_text[random.randint(0, 31000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_symbols = {'\\n', ':', ';', '=', '>', '?', '@', ' ', '!', '\"', '#', '$', '%', '&', '\\'', '*', '+', ',', '\\-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Ё', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Ъ', 'Ы', 'Ь', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё'}\n",
    "\n",
    "allowed_symbols2 = {'\\n', ':', ';', '=', '>', '?', '@', ' ', '!', '\"', '#', '$', '%', '&', '\\'', '*', '+', ',', '\\-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Ё', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Ъ', 'Ы', 'Ь', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё'}\n",
    "\n",
    "allowed_symbols2_rus = {':', ';', '?', '@', ' ', '!', '\"', '#', '$', '%', '*', '+', ',', '\\-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9','Ё', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Ъ', 'Ы', 'Ь', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_MAX_TOKEN_LEN = 5\n",
    "GLOBAL_MAX_TOKEN_COUNT = 253"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 4. RNN\n",
    "## Задача 1. 3 балла\n",
    "Обучите RNN/LSTM на данных из классной работы, используя другой токенайзер. Опишите его и свой выбор. Покажите разницу в генерации моделей, обученных с разными токенайзерами.\n",
    "## {*} Задача 1.1 2 балла\n",
    "Напишите свой токенайзер вручную, с использованием только библиотек numpy, torch, sklearn, stats, опционально других пакетов, не предоставляющих готовые инструменты токенизации и т.п., за исключением предобработки текста (лемматизация, стеминг и т.д.) . \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Tokenizer:\n",
    "    def __init__(self, cut_text, max_count_token: int = 512, max_token_len = 2, allowed_s = allowed_symbols2_rus):\n",
    "        self.allowed_s = allowed_s\n",
    "        self.max_token_len = max_token_len\n",
    "        self.text = cut_text\n",
    "        self.max_count_token = max_count_token\n",
    "        self.specials = {'<pad>':999999, '<bos>':999999, '<eos>':999999}\n",
    "        \n",
    "        self.create_vocab(\"\".join(self.text))\n",
    "        \n",
    "    def token_compare(self, item1:dict, item2:dict):\n",
    "        if ((\" \" in item1[0] or \" \" in item2[0]) and not(item1[0] == \" \" or item2[0] == \" \")):\n",
    "            #print(item1[0], item2[0])\n",
    "            if (\" \" in item1[0] and \" \" in item2[0]):\n",
    "                return 0\n",
    "            elif (\" \" in item1[0]):\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        else:\n",
    "            #if (len(item2[0]) > len(item1[0]) and item2[0])\n",
    "            return item2[1] - item1[1]\n",
    "    \n",
    "    def create_vocab(self, text: str):\n",
    "        regex = r'[^' + \"\".join(self.allowed_s).lower() + ']'\n",
    "        reg = re.compile(regex)\n",
    "        text = reg.sub('', text.replace('ё', 'е'))\n",
    "    \n",
    "        my_dict = {}\n",
    "        i = 1\n",
    "        text_len = len(text)\n",
    "        for index in range(0, text_len - i + 1):\n",
    "            token = text[index:index+i]\n",
    "            if not token in my_dict:\n",
    "                my_dict[token]=9999999\n",
    "    \n",
    "        text = re.sub(r'[^а-я ]', '', text)\n",
    "\n",
    "    \n",
    "        for i in range (2, self.max_token_len+1):\n",
    "            text_len = len(text)\n",
    "            for index in range(0, text_len - i + 1):\n",
    "                token = text[index:index+i]\n",
    "                if token in my_dict:\n",
    "                    my_dict[token]+=1\n",
    "                else:\n",
    "                    my_dict[token]=1\n",
    "                # Assign unique IDs to tokens\n",
    "        \n",
    "        # отсортировали по принципу с пробелом в конец, 1 символ в начало,\n",
    "        #   остальное по частоте, чтобы лего сделать crop\n",
    "        sorted_tokens = dict(sorted(my_dict.items(), key=cmp_to_key(self.token_compare))[:self.max_count_token])\n",
    "        # Сортируем по длине токена, а после по частоте\n",
    "        sorted_tokens.update(self.specials)\n",
    "\n",
    "        sorted_tokens = sorted(sorted_tokens.items(), key=lambda x: (-len(x[0]), -x[1]))\n",
    "\n",
    "        self.token_to_id = {token: idx for idx, (token, _) in enumerate(sorted_tokens)}\n",
    "        self.id_to_token = {idx: token for token, idx in self.token_to_id.items()}\n",
    "    \n",
    "    def _add_special(self, symbol) -> None:\n",
    "        # add special characters to yuor dicts\n",
    "        sym_num = len(self.token_to_id)\n",
    "        self.token_to_id[symbol] = sym_num\n",
    "        self.id_to_token[sym_num] = symbol\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.token_to_id) # your code\n",
    "    \n",
    "    def decode_symbol(self, el):\n",
    "        return self.id_to_token[el]\n",
    "        \n",
    "    def encode_symbol(self, el):\n",
    "        return self.token_to_id[el]\n",
    "\n",
    "    #@property\n",
    "    def tokenize(self, text: str):\n",
    "        \"\"\"\n",
    "        Tokenizes the input text into numbers using the built dictionary.\n",
    "        :param text: Input text to tokenize.\n",
    "        :return: A list of token IDs representing the text.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.token_to_id:\n",
    "            raise ValueError(\"Tokenizer dictionary is empty. Call 'build_dict' first.\")\n",
    "        \n",
    "        tokens = []\n",
    "        i = 0\n",
    "        while i < len(text):\n",
    "            match = None\n",
    "            # Try to find the longest matching token\n",
    "            for size in range(max(self.max_token_len, 5), 0, -1):\n",
    "                if i + size <= len(text):\n",
    "                    substring = text[i:i + size]\n",
    "                    if substring in self.token_to_id:\n",
    "                        match = substring\n",
    "                        break\n",
    "            \n",
    "            if match:\n",
    "                tokens.append(self.token_to_id[match])\n",
    "                i += len(match)  # Move past the matched token\n",
    "            else:\n",
    "                # If no match, handle single characters (fallback)\n",
    "                tokens.append(self.token_to_id[text[i]])\n",
    "                i += 1\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def detokenize(self, token_ids):\n",
    "        \"\"\"\n",
    "        Converts token IDs back into the original text using the dictionary.\n",
    "        :param token_ids: A list of token IDs.\n",
    "        :return: The reconstructed text.\n",
    "        \"\"\"\n",
    "        return ''.join(self.id_to_token[token_id] for token_id in token_ids)\n",
    "    \n",
    "    def encode(self, chars, eos=True):\n",
    "        regex = r'[^' + \"\".join(self.allowed_s) + ']'\n",
    "        reg = re.compile(regex)\n",
    "        chars = reg.sub('', chars.lower().replace('ё', 'е'))\n",
    "\n",
    "        if eos:\n",
    "            chars = '<bos>' + chars + '<eos>'\n",
    "        else:\n",
    "            chars = '<bos>' + chars\n",
    "        return self.tokenize(chars)\n",
    "\n",
    "    def decode(self, idx):\n",
    "        return self.detokenize(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "My_CT = Custom_Tokenizer(cut_text,\n",
    "                 max_count_token=253,\n",
    "                 max_token_len = GLOBAL_MAX_TOKEN_LEN,\n",
    "                 allowed_s = allowed_symbols2_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Dictionary: {'<pad>': 0, '<bos>': 1, '<eos>': 2, 'что': 3, 'ать': 4, 'ает': 5, 'про': 6, 'ост': 7, 'ого': 8, 'оро': 9, 'ста': 10, 'тся': 11, 'оль': 12, 'его': 13, 'ить': 14, 'при': 15, 'мен': 16, 'как': 17, 'ово': 18, 'ест': 19, 'это': 20, 'ени': 21, 'сто': 22, 'ото': 23, 'льн': 24, 'тел': 25, 'ере': 26, 'жен': 27, 'тор': 28, 'все': 29, 'ите': 30, 'оди': 31, 'так': 32, 'гда': 33, 'раз': 34, 'пер': 35, 'ова': 36, 'то': 37, 'на': 38, 'ст': 39, 'не': 40, 'ра': 41, 'ет': 42, 'но': 43, 'по': 44, 'ко': 45, 'ен': 46, 'ро': 47, 'ка': 48, 'ов': 49, 'ть': 50, 'ни': 51, 'те': 52, 'ит': 53, 'от': 54, 'ва': 55, 'го': 56, 'ли': 57, 'ор': 58, 'та': 59, 'во': 60, 'ал': 61, 'ре': 62, 'ер': 63, 'ол': 64, 'пр': 65, 'ос': 66, 'де': 67, 'од': 68, 'ат': 69, 'ом': 70, 'ак': 71, 'ел': 72, 'ла': 73, 'ри': 74, 'ог': 75, 'ин': 76, 'да': 77, 'ло': 78, 'за': 79, 'он': 80, 'ль': 81, 'ле': 82, 'ан': 83, 'че': 84, 'ем': 85, 'ас': 86, 'ил': 87, 'ес': 88, 'ве': 89, 'же': 90, 'ме': 91, 'ся': 92, 'ой': 93, 'мо': 94, 'се': 95, 'до': 96, 'ам': 97, 'ти': 98, 'ма': 99, 'тр': 100, 'ки': 101, 'об': 102, 'чт': 103, 'ди': 104, 'ск': 105, 'ае': 106, 'ны': 107, 'ар': 108, 'ик': 109, 'му': 110, 'ав': 111, 'аз': 112, 'вы': 113, 'ру': 114, 'ед': 115, 'бо': 116, 'оч': 117, 'сл': 118, 'ви': 119, 'ку': 120, 'хо': 121, 'ег': 122, 'со': 123, 'ми': 124, 'чи': 125, 'бы': 126, 'ив': 127, 'уж': 128, 'им': 129, 'ты': 130, 'ок': 131, 'па': 132, 'ис': 133, 'тс': 134, 'си': 135, 'ад': 136, 'вс': 137, 'ие': 138, 'ож': 139, 'пе': 140, 'бе': 141, 'ей': 142, 'ое': 143, 'уд': 144, 'ая': 145, 'из': 146, 'ут': 147, 'ну': 148, 'жи': 149, 'ня': 150, 'ше': 151, 'ча': 152, 'ев': 153, 'ый': 154, 'тв': 155, 'дн': 156, 'ду': 157, 'ят': 158, 'эт': 159, 'аю': 160, 'сь': 161, 'ек': 162, 'нн': 163, 'сп': 164, 'еб': 165, 'зн': 166, 'кр': 167, 'нь': 168, 'са': 169, 'ту': 170, 'пи': 171, 'гд': 172, 'га': 173, 'ьн': 174, 'ля': 175, 'аш': 176, 'ид': 177, 'бу': 178, 'еш': 179, 'оп': 180, 'ши': 181, 'аб': 182, 'шь': 183, 'мн': 184, 'их': 185, 'ир': 186, 'ют': 187, 'ус': 188, 'ез': 189, 'лу': 190, 'лю': 191, 'сс': 192, 'ач': 193, 'ры': 194, 'ия': 195, 'нт': 196, 'ьк': 197, 'о': 198, 'л': 199, 'ь': 200, 'к': 201, ' ': 202, 'з': 203, 'а': 204, 'м': 205, 'е': 206, 'т': 207, 'и': 208, ',': 209, 'ч': 210, 'с': 211, 'в': 212, '\"': 213, 'п': 214, '@': 215, 'р': 216, 'н': 217, 'б': 218, 'я': 219, 'ц': 220, 'ы': 221, 'ш': 222, '.': 223, 'д': 224, 'у': 225, '!': 226, 'г': 227, 'й': 228, 'х': 229, 'ж': 230, '-': 231, 'ю': 232, 'щ': 233, '?': 234, ':': 235, 'э': 236, 'ф': 237, '1': 238, '0': 239, '3': 240, '$': 241, '8': 242, '2': 243, 'ъ': 244, '9': 245, '%': 246, '5': 247, '4': 248, '7': 249, '6': 250, '/': 251, '+': 252, '#': 253, ';': 254, '*': 255}\n"
     ]
    }
   ],
   "source": [
    "print(\"Token Dictionary:\", My_CT.token_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text1 = \"Хозяин: Великолепный поступок. А ты знаешь, что всего только раз в жизни выпадает влюбленным день, когда все им удается?. И ты прозевал свое счастье. Прощай. Я больше не буду тебе помогать. Нет! Мешать начну изо всех сил.\"\n",
    "test_text2 = \"Правосудие - продано\"\n",
    "\n",
    "tokenized_text = My_CT.encode(test_text1)\n",
    "backed_text =  My_CT.decode(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos> хо з я ин :   ве ли ко ле п ны й   по ст у по к .   а   ты   зн ае шь ,   что   все го   то ль ко   раз   в   жи зн и   вы па да ет   в лю б ле нн ы м   де нь ,   ко гда   все   им   уд ает ся ? .   и   ты   про з ев ал   с во е   с ча ст ь е .   про щ а й .   я   бо ль ше   не   бу ду   те бе   по мо га ть .   не т !   ме ш ать   на ч ну   из о   все х   си л . <eos> "
     ]
    }
   ],
   "source": [
    "for i in tokenized_text:\n",
    "    print(My_CT.detokenize([i]), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>хозяин: великолепный поступок. а ты знаешь, что всего только раз в жизни выпадает влюбленным день, когда все им удается?. и ты прозевал свое счастье. прощай. я больше не буду тебе помогать. нет! мешать начну изо всех сил.<eos>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JokesDataset(Dataset):\n",
    "    def __init__(self, tokenizer, cut_text, max_text_len:int = 512):\n",
    "        self.max_text_len = max_text_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cut_text = cut_text\n",
    "        self.pad_index = self.tokenizer.encode_symbol(\"<pad>\")\n",
    "\n",
    "        self.complited = []\n",
    "        # Сделаем человеческий Dataset: обработку вытащим в инициализацию:\n",
    "        #  в идеале запонлять паддингами лучше в другом месте\n",
    "        for item in self.cut_text:\n",
    "            encoded = self.tokenizer.encode(item)[:self.max_text_len]\n",
    "            padded = torch.full((self.max_text_len, ), self.pad_index, dtype=torch.long)\n",
    "            padded[:len(encoded)] = torch.tensor(encoded)\n",
    "            self.complited.append(padded)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cut_text)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        ex = self.complited[item]\n",
    "        # pad your sequence and make a final sample. You can skip padding and pad sequences with torch special method.\n",
    "        return ex, len(ex)\n",
    "\n",
    "# Optionally add new methods to your dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        hidden_dim: int = 256,\n",
    "        num_layers: int = 2,\n",
    "        drop_prob: float = 0.5,\n",
    "        max_text_len: int = 512, # Максимальная длина сообщения в символах\n",
    "    ) -> None:\n",
    "        '''    hidden_dim: Длинна словаря (количество ожидаемых признаков во входных данных x)\n",
    "               max_len: Максимальная длина сообщения в символах\n",
    "        '''\n",
    "        \n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.max_text_len = max_text_len\n",
    "        # create mappings\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        ## define the LSTM, dropout and fully connected layers\n",
    "        self.encoder = nn.Embedding(self.tokenizer.vocab_size, self.hidden_dim)\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.hidden_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=self.drop_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=self.drop_prob)\n",
    "        self.decoder = nn.Linear(\n",
    "            in_features=self.hidden_dim,\n",
    "            out_features=self.tokenizer.vocab_size,\n",
    "        )\n",
    "\n",
    "    # Forward - это проход вперёд по слою\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, lengths: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # one-hot encode your sequence\n",
    "        packed_embeds = self.encoder(x) # pack your sequence. This helps with the efficiency. Use torch function pack_padded_sequence\n",
    "        outputs, hidden = self.rnn(packed_embeds) # run you model\n",
    "        \n",
    "        # TODO: Понять нафига\n",
    "        #  out, lengths = # pad sequence back\n",
    "        \n",
    "        # Pass through a dropout layer and fully connected layer\n",
    "        out = self.dropout(outputs)\n",
    "        ## Get the output for classification.\n",
    "        out = self.decoder(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    \n",
    "    # инференс - режим не обучения (По сути штатная работа)\n",
    "    def inference(self, prefix='<bos> ', device=\"cpu\"):\n",
    "        tokens = torch.tensor([self.tokenizer.encode(prefix, eos=False)], device=device) # encode prefix\n",
    "        \n",
    "        # 2 stopping conditions: reaching max len or getting <eos> token\n",
    "        # Generate sequence iteratively\n",
    "        for _ in range(self.max_text_len - len(tokens[0])):\n",
    "            # YOUR CODE: generate sequence one by one\n",
    "            # Pass tokens through the embedding layer\n",
    "            logits, hidden = self.forward(tokens, torch.tensor([tokens.size(1)]))\n",
    "            \n",
    "            # Get the last token's logits and sample a token\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            new_token = torch.multinomial(\n",
    "                torch.nn.functional.softmax(next_token_logits, dim=-1), num_samples=1\n",
    "            )\n",
    "\n",
    "            # Append the new token\n",
    "            tokens = torch.cat([tokens, new_token], dim=1)\n",
    "\n",
    "            # Stop if the <eos> token is generated\n",
    "            if new_token.item() == self.tokenizer.encode_symbol(\"<eos>\"):\n",
    "                break\n",
    "        # Decode the token IDs back into a string\n",
    "        return self.tokenizer.decode(tokens.squeeze().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(\n",
    "    model: CharRNN,\n",
    "    train_batch: Tuple[torch.Tensor, torch.Tensor],\n",
    "    vocab_size: int,\n",
    "    criterion: nn.Module,\n",
    "    optimizer,\n",
    "    device=\"cpu\"\n",
    ") -> torch.Tensor:\n",
    "    inputs, lengths = train_batch\n",
    "    inputs = inputs.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "\n",
    "    # Сброс градиентов\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Прямой проход\n",
    "    outputs, _ = model(inputs[:, :-1], lengths)\n",
    "\n",
    "    # Переформатирование выходов и целевых меток для расчета функции потерь\n",
    "    outputs = outputs.view(-1, vocab_size)\n",
    "    targets = inputs[:, 1:].reshape(-1)\n",
    "\n",
    "    # Вычисление функции потерь\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Обратный проход\n",
    "    loss.backward()\n",
    "\n",
    "    # Шаг оптимизации\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(losses):\n",
    "    clear_output()\n",
    "    plt.plot(range(1, len(losses) + 1), losses)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_hidden = 512 # 10+3 #64 #256\n",
    "n_layers = 4 #4\n",
    "drop_prob = 0.1\n",
    "lr = 0.001\n",
    "\n",
    "num_epochs = 3\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Custom_Tokenizer(cut_text,\n",
    "                             max_count_token=GLOBAL_MAX_TOKEN_COUNT,\n",
    "                             max_token_len = GLOBAL_MAX_TOKEN_LEN,\n",
    "                             allowed_s = allowed_symbols2_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRNN(tokenizer, hidden_dim=n_hidden, num_layers=n_layers, drop_prob=drop_prob)\n",
    "hidden = None\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = JokesDataset(tokenizer, cut_text, 256)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1242 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.0131 :  48%|████▊     | 600/1242 [08:55<09:32,  1.12it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)) \u001b[38;5;28;01mas\u001b[39;00m prbar:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m----> 7\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m         epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;66;03m#print(f'Done {i/len(dataloader) * 100:.2f}%, Loss: {loss:.4f}')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 17\u001b[0m, in \u001b[0;36mtraining_step\u001b[0;34m(model, train_batch, vocab_size, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Прямой проход\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m outputs, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Переформатирование выходов и целевых меток для расчета функции потерь\u001b[39;00m\n\u001b[1;32m     20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size)\n",
      "File \u001b[0;32m~/Документы/Першин_Никольская_Нейронные_сети/.myvenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Документы/Першин_Никольская_Нейронные_сети/.myvenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 44\u001b[0m, in \u001b[0;36mCharRNN.forward\u001b[0;34m(self, x, lengths)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, lengths: torch\u001b[38;5;241m.\u001b[39mTensor\n\u001b[1;32m     41\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# one-hot encode your sequence\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     packed_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x) \u001b[38;5;66;03m# pack your sequence. This helps with the efficiency. Use torch function pack_padded_sequence\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     outputs, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked_embeds\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# run you model\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# TODO: Понять нафига\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m#  out, lengths = # pad sequence back\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# Pass through a dropout layer and fully connected layer\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(outputs)\n",
      "File \u001b[0;32m~/Документы/Першин_Никольская_Нейронные_сети/.myvenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Документы/Першин_Никольская_Нейронные_сети/.myvenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Документы/Першин_Никольская_Нейронные_сети/.myvenv/lib/python3.12/site-packages/torch/nn/modules/rnn.py:917\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    914\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 917\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    920\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    921\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_loss = 0\n",
    "    with tqdm.tqdm(total=len(dataloader)) as prbar:\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            loss = training_step(model, batch, tokenizer.vocab_size, criterion, optimizer, device)\n",
    "            epoch_loss += loss\n",
    "        \n",
    "            if i % 100 == 0:\n",
    "                #print(f'Done {i/len(dataloader) * 100:.2f}%, Loss: {loss:.4f}')\n",
    "                metrics_str = f\"Loss: {round(loss, 4)} \"\n",
    "                #for k, v in metrics_dict.items():\n",
    "                #    metrics_str += f\"{k}: {round(float(v), 4)} \"\n",
    "                prbar.set_description(metrics_str)\n",
    "                prbar.update(100)\n",
    "    \n",
    "    epoch_loss /= len(dataloader)\n",
    "    losses.append(epoch_loss)\n",
    "    \n",
    "    plot_losses(losses)\n",
    "    #torch.save(model.state_dict(), \"rnn.pt\")\n",
    "    torch.save(model, \"rnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(\"rnn.pt\", weights_only=True))\n",
    "model = torch.load(\"rnn.pt\", weights_only=False)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<bos>пилите, шура, пилите. они теждий алерса!<eos>',\n",
       " '<bos>пилите, шура, пилите. они дотибь, не при нелоповидиет!<eos>',\n",
       " '<bos>пилите, шура, пилите. они \"умолонысо крмем дреб.?<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " '<bos>пилите, шура, пилите. они вочанмькие.<eos>',\n",
       " '<bos>пилите, шура, пилите. они вессвкацься. естенинто вснагак.ой распулитлаю доколоне не медизар.<eos>',\n",
       " '<bos>пилите, шура, пилите. они к, козьк,ся кролю из когай и не ужы.<eos>',\n",
       " '<bos>пилите, шура, пилите. они хорит, в сивемока! это бым упесостесям...<eos>',\n",
       " '<bos>пилите, шура, пилите. они телодочо в илентравиць обяденон:<eos>',\n",
       " '<bos>пилите, шура, пилите. они зной изчелька вснь кутога.<eos>',\n",
       " '<bos>пилите, шура, пилите. они  пруметы, набы, навечал, мнсяы куболицая в стенугноцяциль.<eos>']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[model.inference(\"Пилите, Шура, пилите. Они \", device=device) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 2. 3 балла\n",
    "Реализуйте с помощью только torch/numpy слой RNN, обучите его на данных из классной работы и, опционально, своих данных. Покажите, что модель обучается\n",
    "## {*} Задача 2.1 +1 балл\n",
    "За реализацию слоев GRU/LSTM/bidirectional RNN, многослойной модели по +1 баллу к базовым (даже если ванильная RNN не реализована)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 3. 1/2/3/4 балла\n",
    "**TBD**: \n",
    "Попробуйте обучить рекуррентную сеть задаче классификации. Вы можете воспользоваться сторонними библиотеками для вашей работы, \n",
    "но модель и основной код должны быть написаны на pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  {*} Задача 4. 5/6/7/8 баллов\n",
    "[ссылка](https://www.kaggle.com/t/b2ef08dc3ddf44f981e2ad186c6c508d)\n",
    "\n",
    "Попробуйте обучить сверточную нейронную сеть задаче детекции людей на изображениях разного стиля. Вы можете воспользоваться сторонними библиотеками для вашей работы. Однако, за неисопользование полностью готовых скриптов обучения (как в классной работе) вы получите дополнительные2 балла"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
