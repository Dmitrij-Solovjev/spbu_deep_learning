{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Iterable, Tuple\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.distributions.categorical import Categorical\n",
    "import tqdm\n",
    "import re\n",
    "from functools import cmp_to_key\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "with open(\"../../datasets/anek_djvu.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|startoftext|>Друзья мои, чтобы соответствовать вам, я готов сделать над собой усилие и стать лучше. Но тогда и вы станьте немного хуже!\\n\\n<|startoftext|>- Люся, ты все еще хранишь мой подарок?- Да.- Я думал, ты выкинула все, что со мной связано.- Плюшевый мишка не виноват, что ты ебл@н...\\n\\n<|startoftext|>- А вот скажи честно, ты во сне храпишь?- Понятие не имею, вроде, нет. От со'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[118:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Разбитие на анектоды\n",
    "def cut_data(text):\n",
    "    return text.replace(\"\\n\\n\", \"\").split(\"<|startoftext|>\")[1:]\n",
    "cut_text = cut_data(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ученые - это такие люди, которые способны назвать точный химический состав Солнца, но не могут определить все ингредиенты Кока-колы.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "cut_text[random.randint(0, 31000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_symbols = {'\\n', ':', ';', '=', '>', '?', '@', ' ', '!', '\"', '#', '$', '%', '&', '\\'', '*', '+', ',', '\\-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Ё', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Ъ', 'Ы', 'Ь', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё'}\n",
    "\n",
    "allowed_symbols2 = {'\\n', ':', ';', '=', '>', '?', '@', ' ', '!', '\"', '#', '$', '%', '&', '\\'', '*', '+', ',', '\\-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Ё', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Ъ', 'Ы', 'Ь', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё'}\n",
    "\n",
    "allowed_symbols2_rus = {':', ';', '?', '@', ' ', '!', '\"', '#', '$', '%', '*', '+', ',', '\\-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9','Ё', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Ъ', 'Ы', 'Ь', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё'}\n",
    "\n",
    "regex = r'[^' + \"\".join(allowed_symbols) + ']'\n",
    "\n",
    "import re\n",
    "from functools import cmp_to_key\n",
    "\n",
    "def compare(item1:dict, item2:dict):\n",
    "    if ((\" \" in item1[0] or \" \" in item2[0]) and not(item1[0] == \" \" or item2[0] == \" \")):\n",
    "        #print(item1[0], item2[0])\n",
    "        if (\" \" in item1[0] and \" \" in item2[0]):\n",
    "            return 0\n",
    "        elif (\" \" in item1[0]):\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    else:\n",
    "        return item2[1] - item1[1]\n",
    "\n",
    "\n",
    "#re.sub(regex, None, \"У кота была собака %^&*()\", flags = re.IGNORECASE)\n",
    "def Subsequence_counter(text: str, max_len = 10, allowed_s = allowed_symbols2_rus, get_first_num = 512):\n",
    "    regex = r'[^' + \"\".join(allowed_s).lower() + ']'\n",
    "    reg = re.compile(regex)\n",
    "    text = reg.sub('', text.replace('ё', 'е'))\n",
    "    \n",
    "    my_dict = {}\n",
    "    i = 1\n",
    "    text_len = len(text)\n",
    "    for index in range(0, text_len - i + 1):\n",
    "        token = text[index:index+i]\n",
    "        if not token in my_dict:\n",
    "            my_dict[token]=9999999\n",
    "    \n",
    "    text = re.sub(r'[^а-я ]', '', text)\n",
    "\n",
    "    \n",
    "    for i in range (2, max_len):\n",
    "        text_len = len(text)\n",
    "        for index in range(0, text_len - i + 1):\n",
    "            token = text[index:index+i]\n",
    "            if token in my_dict:\n",
    "                my_dict[token]+=1\n",
    "            else:\n",
    "                my_dict[token]=1\n",
    "    return sorted(my_dict.items(), key=cmp_to_key(compare)) #[:get_first_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('о', 9999999), ('л', 9999999), ('ь', 9999999), ('к', 9999999), (' ', 9999999), ('з', 9999999), ('а', 9999999), ('м', 9999999), ('е', 9999999), ('т', 9999999), ('и', 9999999), (',', 9999999), ('ч', 9999999), ('с', 9999999), ('в', 9999999), ('\"', 9999999), ('п', 9999999), ('@', 9999999), ('р', 9999999), ('н', 9999999), ('б', 9999999), ('я', 9999999), ('ц', 9999999), ('ы', 9999999), ('ш', 9999999), ('.', 9999999), ('д', 9999999), ('у', 9999999), ('!', 9999999), ('г', 9999999), ('й', 9999999), ('х', 9999999), ('ж', 9999999), ('-', 9999999), ('ю', 9999999), ('щ', 9999999), ('?', 9999999), (':', 9999999), ('э', 9999999), ('ф', 9999999), ('1', 9999999), ('0', 9999999), ('3', 9999999), ('$', 9999999), ('8', 9999999), ('2', 9999999), ('ъ', 9999999), ('9', 9999999), ('%', 9999999), ('5', 9999999), ('4', 9999999), ('7', 9999999), ('6', 9999999), ('/', 9999999), ('+', 9999999), ('#', 9999999), (';', 9999999), ('*', 9999999), ('то', 161732), ('на', 133490), ('ст', 112896), ('не', 107012), ('ра', 106863), ('ет', 104163), ('но', 103662), ('по', 102777), ('ко', 100215), ('ен', 99223), ('ро', 91635), ('ка', 89021), ('ов', 83452), ('ть', 82466), ('ни', 81897), ('те', 78576), ('ит', 77654), ('от', 77011), ('ва', 76039), ('го', 75793), ('ли', 75048), ('ор', 73484), ('та', 73283), ('во', 72339), ('ал', 72109), ('ре', 71826), ('ер', 70579), ('ол', 70043), ('пр', 69134), ('ос', 68246), ('де', 66502), ('од', 65624), ('ат', 64293), ('ом', 61886), ('ак', 61453), ('ел', 59353), ('ла', 58070), ('ри', 57961), ('ог', 57546), ('ин', 56157), ('да', 55936), ('ло', 54705), ('за', 53604), ('он', 52548), ('ль', 52419), ('ле', 51276), ('ан', 50021), ('че', 49261), ('ем', 49149), ('ас', 48569), ('ил', 48281), ('ес', 47956), ('ве', 47189), ('же', 46987), ('ме', 46985), ('ся', 46869), ('ой', 46620), ('мо', 46273), ('се', 43640), ('до', 43473), ('ам', 43217), ('ти', 43013), ('ма', 42375), ('тр', 41397), ('ки', 41150), ('об', 40987), ('чт', 40085), ('ди', 39633), ('ск', 39338), ('ае', 39090), ('ны', 39051), ('ар', 38207), ('ик', 35801), ('му', 35277), ('ав', 34875), ('аз', 34842), ('вы', 34547), ('ру', 33271), ('ед', 33184), ('бо', 32253), ('оч', 31551), ('сл', 31114), ('ви', 30603), ('ку', 30522), ('хо', 30290), ('ег', 29757), ('со', 29477), ('ми', 28776), ('чи', 28683), ('бы', 28206), ('ив', 28186), ('уж', 27846), ('им', 27648), ('ты', 27179), ('ок', 26808), ('па', 26756), ('ис', 26724), ('тс', 26187), ('си', 26131), ('ад', 25610), ('вс', 25170), ('ие', 25150), ('ож', 25025), ('пе', 24977), ('бе', 24688), ('ей', 24666), ('ое', 24516), ('уд', 24251), ('ая', 24005), ('из', 23807), ('ут', 23743), ('ну', 23529), ('жи', 23359), ('ня', 23310), ('ше', 23015), ('ча', 22648), ('ев', 22635), ('ый', 22213), ('тв', 22157), ('дн', 21926), ('ду', 21924), ('ят', 21923), ('эт', 21588), ('аю', 21372), ('сь', 20930), ('ек', 20382), ('нн', 19996), ('сп', 19989), ('еб', 19716), ('зн', 19645), ('кр', 19621), ('нь', 19060), ('са', 18798), ('ту', 18674), ('пи', 18602), ('гд', 18553), ('га', 18506), ('ьн', 18490), ('ля', 18466), ('аш', 18349), ('ид', 18186), ('бу', 18123), ('еш', 17874), ('оп', 17863), ('ши', 17838), ('аб', 17833), ('шь', 17312), ('мн', 17223), ('их', 17128), ('ир', 17067), ('ют', 17055), ('ус', 17032), ('ез', 16894), ('лу', 16465), ('лю', 16390), ('сс', 16254), ('ач', 15975), ('ры', 15724), ('ия', 15460), ('нт', 15129), ('ьк', 15038), ('аж', 14817), ('ай', 14793), ('ще', 14791), ('ба', 14782), ('ий', 14750), ('ич', 14593), ('оз', 14589), ('ые', 14228), ('ап', 14220), ('би', 14205), ('уч', 13931), ('ее', 13716), ('щи', 13712), ('св', 13656), ('кт', 13438), ('ьс', 12975), ('шк', 12713), ('др', 12639), ('ум', 12573), ('тн', 12568), ('гр', 12515), ('еч', 12484), ('ву', 12475), ('ыв', 12352), ('ыл', 12245), ('уш', 12223), ('ую', 12198), ('жа', 12045), ('ке', 11976), ('ош', 11961), ('вн', 11891), ('ии', 11798), ('бр', 11783), ('ур', 11770), ('уп', 11592), ('сн', 11572), ('пл', 11453), ('иц', 11379), ('мы', 11305), ('уг', 11216), ('жн', 11154), ('ци', 10904), ('ша', 10793), ('ых', 10690), ('рн', 10679), ('ах', 10624), ('ои', 10503), ('чн', 10471), ('ул', 10433), ('ги', 10393), ('ря', 10383), ('вр', 10251), ('дв', 10168), ('бл', 10081), ('иш', 9942), ('чк', 9934), ('тк', 9691), ('см', 9494), ('рт', 9491), ('жд', 9428), ('еп', 9420), ('гл', 9389), ('бя', 9331), ('це', 9319), ('зв', 9238), ('су', 9168), ('нс', 9153), ('ым', 9062), ('зд', 8989), ('ук', 8967), ('кл', 8533), ('ещ', 8353), ('лс', 8118), ('нк', 8079), ('уб', 8024), ('вл', 7991), ('пу', 7988), ('лы', 7871), ('гу', 7869), ('зо', 7812), ('оя', 7763), ('ды', 7760), ('еж', 7591), ('иг', 7539), ('дл', 7396), ('зи', 7320), ('ох', 7291), ('аг', 7273), ('юб', 7260), ('ьш', 7153), ('ца', 6982), ('ха', 6858), ('йт', 6823), ('ыт', 6745), ('йс', 6721), ('ио', 6689), ('ян', 6630), ('сы', 6602), ('дь', 6478), ('чу', 6464), ('зы', 6460), ('ех', 6426), ('рь', 6304), ('ье', 6219), ('нщ', 6179), ('лк', 6009), ('рв', 5797), ('ыс', 5765), ('нд', 5741), ('иб', 5617), ('иа', 5562), ('рм', 5546), ('ял', 5541), ('вт', 5420), ('фи', 5419), ('кс', 5334), ('рк', 5296), ('дя', 5284), ('ья', 5274), ('мя', 5266), ('ью', 5217), ('ща', 5192), ('ух', 5171), ('юд', 5092), ('шл', 5085), ('уе', 5054), ('ец', 4962), ('тя', 4912), ('ыш', 4839), ('зу', 4837), ('лл', 4769), ('фо', 4685), ('ию', 4654), ('яе', 4634), ('ув', 4634), ('дк', 4561), ('оо', 4543), ('жу', 4489), ('зе', 4484), ('яд', 4480), ('вк', 4407), ('шо', 4393), ('ын', 4355), ('уз', 4336), ('ге', 4237), ('нц', 4223), ('зм', 4193), ('мп', 4163), ('дс', 4119), ('жч', 4094), ('ац', 3980), ('йн', 3939), ('рс', 3931), ('аи', 3850), ('шн', 3843), ('ям', 3798), ('ун', 3781), ('ср', 3684), ('яв', 3645), ('ыб', 3629), ('сч', 3616), ('ео', 3591), ('зя', 3589), ('ьм', 3515), ('тл', 3492), ('зр', 3444), ('чь', 3427), ('ющ', 3393), ('пы', 3384), ('яс', 3366), ('ип', 3363), ('шу', 3362), ('кв', 3316), ('пя', 3288), ('сд', 3272), ('ащ', 3258), ('ыр', 3256), ('лн', 3224), ('оф', 3197), ('рд', 3188), ('еа', 3175), ('ьт', 3153), ('фе', 3140), ('ып', 3137), ('чш', 3115), ('пь', 3107), ('йд', 3056), ('вз', 3030), ('бщ', 2995), ('яз', 2993), ('зг', 2990), ('уа', 2983), ('юс', 2938), ('вш', 2906), ('рж', 2906), ('ою', 2863), ('бн', 2854), ('иж', 2846), ('рг', 2750), ('кн', 2746), ('хи', 2713), ('уй', 2709), ('ык', 2699), ('вя', 2696), ('ущ', 2687), ('цы', 2662), ('ящ', 2627), ('лж', 2602), ('хн', 2569), ('ау', 2568), ('вь', 2561), ('ао', 2476), ('зл', 2424), ('рю', 2410), ('вд', 2349), ('цу', 2339), ('нг', 2317), ('йч', 2302), ('ьг', 2264), ('яю', 2261), ('ищ', 2244), ('хр', 2234), ('нч', 2190), ('аф', 2184), ('аа', 2183), ('ню', 2182), ('еф', 2161), ('мс', 2097), ('ху', 2073), ('ьз', 2072), ('тд', 2064), ('гн', 2062), ('рш', 2056), ('бк', 2051), ('мм', 2037), ('вч', 2037), ('бъ', 2035), ('зб', 2011), ('ощ', 2002), ('лг', 1989), ('як', 1959), ('юч', 1938), ('фа', 1929), ('зь', 1888), ('ъя', 1861), ('яч', 1848), ('рп', 1848), ('ьо', 1840), ('эк', 1800), ('оц', 1787), ('ьб', 1765), ('ъе', 1762), ('ях', 1760), ('пт', 1757), ('ьч', 1755), ('йк', 1735), ('ыг', 1693), ('хв', 1668), ('жк', 1651), ('ыч', 1646), ('рх', 1625), ('сю', 1616), ('мл', 1607), ('еи', 1585), ('мь', 1585), ('рл', 1583), ('ею', 1572), ('жо', 1554), ('цо', 1528), ('ыд', 1525), ('фр', 1521), ('яж', 1488), ('оу', 1486), ('еу', 1475), ('ея', 1438), ('бс', 1436), ('тб', 1410), ('дц', 1407), ('нр', 1403), ('ьа', 1399), ('кц', 1381), ('йо', 1381), ('вм', 1365), ('хл', 1327), ('тп', 1325), ('щу', 1321), ('сб', 1318), ('пк', 1314), ('сх', 1302), ('оа', 1291), ('яй', 1289), ('дж', 1286), ('йц', 1276), ('фу', 1270), ('мк', 1253), ('яц', 1248), ('зк', 1236), ('яр', 1224), ('яо', 1218), ('иф', 1201), ('шт', 1193), ('дп', 1188), ('вп', 1176), ('рр', 1169), ('пс', 1159), ('рб', 1141), ('дх', 1137), ('пн', 1126), ('тю', 1119), ('яа', 1115), ('йм', 1111), ('лт', 1085), ('пп', 1085), ('лч', 1084), ('оэ', 1038), ('ьц', 1021), ('уо', 1019), ('юр', 1015), ('ьи', 1012), ('хе', 1011), ('тч', 1004), ('ыз', 986), ('яг', 967), ('тт', 959), ('эр', 943), ('лд', 926), ('зж', 925), ('гк', 911), ('эл', 893), ('чо', 892), ('дт', 886), ('цв', 885), ('мб', 882), ('зз', 864), ('иу', 857), ('тм', 838), ('нф', 799), ('жр', 798), ('съ', 794), ('юм', 789), ('лб', 785), ('рч', 780), ('яб', 777), ('ьд', 770), ('йа', 769), ('мр', 747), ('йе', 732), ('яи', 720), ('фт', 712), ('рщ', 691), ('вв', 685), ('юн', 677), ('дм', 674), ('бь', 672), ('фл', 664), ('кз', 659), ('ыо', 657), ('яя', 654), ('бв', 641), ('дд', 638), ('чл', 637), ('нз', 626), ('тц', 617), ('мт', 605), ('ьр', 586), ('ыа', 585), ('сш', 576), ('дш', 551), ('уи', 550), ('йш', 544), ('ыж', 538), ('хм', 537), ('жс', 535), ('яп', 535), ('ьу', 534), ('дб', 512), ('рц', 508), ('ьв', 508), ('дъ', 495), ('юо', 484), ('вц', 481), ('бм', 476), ('рз', 475), ('жь', 473), ('кк', 471), ('юк', 470), ('цк', 467), ('юл', 461), ('ыи', 458), ('юа', 457), ('йл', 455), ('щь', 455), ('уу', 445), ('вх', 445), ('йф', 443), ('кд', 442), ('яу', 435), ('хт', 432), ('юе', 432), ('нв', 429), ('жб', 426), ('дч', 408), ('мч', 403), ('бю', 402), ('мв', 401), ('сф', 397), ('пч', 389), ('юш', 386), ('юю', 382), ('сэ', 377), ('вщ', 367), ('хс', 367), ('бх', 366), ('эн', 360), ('сц', 358), ('кч', 357), ('гч', 356), ('гв', 351), ('йр', 350), ('пц', 347), ('юх', 344), ('сг', 335), ('цт', 332), ('юз', 330), ('йу', 315), ('шр', 311), ('лз', 311), ('щн', 303), ('йб', 302), ('уц', 301), ('уф', 300), ('ыу', 294), ('фф', 288), ('дз', 287), ('ыя', 287), ('рф', 286), ('бб', 285), ('эс', 284), ('уя', 269), ('эф', 267), ('км', 263), ('эп', 263), ('зт', 263), ('юг', 259), ('дг', 255), ('хй', 254), ('тз', 245), ('дю', 245), ('тф', 244), ('шю', 244), ('ьл', 244), ('аы', 243), ('тг', 241), ('шп', 234), ('лп', 233), ('гт', 232), ('шв', 231), ('яш', 229), ('ьп', 225), ('мц', 224), ('сз', 224), ('лр', 223), ('зю', 222), ('бч', 222), ('мж', 221), ('нж', 217), ('ьф', 213), ('ьщ', 211), ('ьы', 209), ('нш', 204), ('йп', 201), ('иы', 201), ('вб', 200), ('аэ', 200), ('лм', 200), ('зп', 195), ('зс', 194), ('йи', 193), ('эм', 191), ('юц', 191), ('жл', 185), ('еы', 182), ('кг', 177), ('хг', 176), ('лв', 169), ('мэ', 166), ('сж', 165), ('йв', 164), ('мд', 162), ('нл', 162), ('цс', 159), ('нб', 157), ('лш', 157), ('зч', 155), ('пз', 154), ('яы', 152), ('тх', 152), ('шм', 149), ('юп', 146), ('кж', 145), ('лф', 145), ('мф', 145), ('вг', 143), ('эв', 140), ('юв', 134), ('оы', 134), ('пю', 131), ('лх', 131), ('юу', 128), ('фы', 126), ('жм', 125), ('уэ', 124), ('фч', 115), ('жг', 115), ('гс', 114), ('юи', 112), ('уы', 111), ('цц', 109), ('зъ', 105), ('бт', 105), ('йз', 104), ('тщ', 100), ('мг', 98), ('йг', 98), ('ыщ', 96), ('пш', 96), ('нх', 96), ('иэ', 96), ('бз', 96), ('йы', 95), ('рэ', 95), ('жж', 94), ('тж', 93), ('нп', 93), ('бж', 89), ('бд', 88), ('жп', 88), ('тэ', 86), ('юж', 85), ('мз', 84), ('кы', 83), ('бг', 82), ('чр', 81), ('эй', 80), ('кю', 79), ('ыы', 78), ('чм', 74), ('въ', 71), ('нэ', 70), ('хя', 69), ('хд', 69), ('фм', 67), ('кб', 67), ('йщ', 64), ('тъ', 64), ('кш', 61), ('кп', 61), ('хч', 60), ('лщ', 58), ('хп', 58), ('йх', 58), ('хб', 56), ('дэ', 55), ('фс', 54), ('эш', 53), ('хз', 52), ('фх', 52), ('мщ', 52), ('мю', 52), ('фш', 52), ('гм', 52), ('юй', 52), ('йя', 51), ('хк', 51), ('эг', 50), ('що', 48), ('бш', 48), ('ьх', 48), ('жв', 47), ('йю', 47), ('хш', 47), ('фн', 47), ('гг', 46), ('жц', 45), ('хы', 45), ('фю', 45), ('зц', 45), ('лэ', 45), ('цн', 45), ('юы', 44), ('цм', 44), ('ыц', 43), ('шс', 43), ('гб', 42), ('эд', 42), ('бц', 41), ('хэ', 39), ('шц', 38), ('пм', 37), ('вю', 36), ('цл', 35), ('пд', 35), ('бэ', 35), ('ьж', 34), ('эб', 31), ('еь', 31), ('цп', 30), ('кэ', 30), ('гы', 30), ('мх', 30), ('цг', 29), ('щр', 29), ('фэ', 28), ('тш', 27), ('эх', 26), ('иь', 26), ('фг', 25), ('кф', 25), ('гп', 24), ('ьь', 24), ('чв', 24), ('ээ', 24), ('нм', 24), ('мш', 24), ('гз', 23), ('цр', 23), ('вэ', 23), ('аь', 23), ('фк', 21), ('фп', 21), ('жт', 21), ('эз', 21), ('шб', 20), ('фь', 20), ('вж', 19), ('еэ', 19), ('ыю', 19), ('хх', 19), ('гш', 18), ('жю', 18), ('чч', 18), ('чп', 18), ('чх', 17), ('зш', 16), ('хю', 16), ('кх', 16), ('оь', 15), ('юф', 14), ('йж', 14), ('лц', 14), ('зэ', 14), ('цд', 14), ('чс', 14), ('яь', 14), ('цб', 13), ('ыь', 12), ('жы', 12), ('дф', 12), ('шы', 12), ('пг', 12), ('уь', 12), ('юя', 12), ('яф', 11), ('шш', 11), ('ыф', 10), ('эя', 10), ('ьй', 10), ('цз', 10), ('цю', 10), ('чэ', 10), ('жэ', 10), ('кя', 10), ('кь', 10), ('цэ', 10), ('пф', 9), ('хф', 9), ('гэ', 9), ('ъб', 9), ('цш', 9), ('гф', 9), ('пщ', 9), ('йь', 8), ('сщ', 8), ('пб', 8), ('нъ', 8), ('пэ', 8), ('ць', 7), ('гю', 7), ('йй', 7), ('щд', 7), ('чы', 7), ('яэ', 6), ('эу', 6), ('эц', 6), ('щю', 6), ('пв', 6), ('хж', 6), ('ьъ', 6), ('юэ', 5), ('чд', 5), ('щт', 5), ('хъ', 5), ('фб', 5), ('оъ', 5), ('цч', 5), ('вф', 5), ('мй', 5), ('юь', 5), ('чг', 5), ('щя', 5), ('тй', 5), ('жз', 5), ('кщ', 5), ('бп', 5), ('фд', 4), ('зф', 4), ('шэ', 4), ('чб', 4), ('ръ', 4), ('ъю', 4), ('зх', 4), ('чю', 4), ('гй', 4), ('чя', 4), ('ьэ', 4), ('пх', 4), ('дщ', 4), ('гя', 4), ('ця', 3), ('еъ', 3), ('ъи', 3), ('йэ', 3), ('зщ', 3), ('щы', 3), ('гц', 3), ('чз', 3), ('цх', 3), ('жщ', 3), ('эи', 3), ('шз', 3), ('ыъ', 3), ('шч', 3), ('эо', 3), ('щщ', 3), ('лй', 3), ('сй', 3), ('уъ', 2), ('хщ', 2), ('иъ', 2), ('нй', 2), ('хц', 2), ('йъ', 2), ('шг', 2), ('аъ', 2), ('жя', 2), ('хь', 2), ('фз', 2), ('эщ', 2), ('щй', 2), ('щм', 2), ('мъ', 2), ('кй', 2), ('эа', 2), ('ыэ', 2), ('юъ', 2), ('шя', 2), ('ъч', 2), ('фя', 2), ('жф', 2), ('бф', 2), ('ъс', 2), ('шх', 2), ('пж', 2), ('гж', 2), ('гх', 2), ('ъу', 1), ('ъа', 1), ('эю', 1), ('чй', 1), ('цй', 1), ('яъ', 1), ('щз', 1), ('ъв', 1), ('эж', 1), ('щж', 1), ('чф', 1), ('шщ', 1), ('къ', 1), ('ъх', 1), ('щл', 1), ('щк', 1), ('эч', 1), ('лъ', 1), ('щс', 1), ('гь', 1), ('щв', 1), ('дй', 1), ('пъ', 1), ('рй', 1), ('гщ', 1), ('цщ', 1), ('чж', 1), ('вй', 1), ('эе', 1), ('о ', 225427), (' з', 55861), ('л ', 43095), (' ч', 63553), (' с', 168149), (' п', 187872), (' н', 178460), ('я ', 135114), ('и ', 175131), (' ц', 4416), (' к', 103017), (' а', 71985), ('к ', 59367), (' в', 167879), ('е ', 231053), (' б', 65547), (' м', 90215), ('ы ', 78152), ('ь ', 125879), ('м ', 78826), (' я', 24083), (' г', 38627), ('в ', 69557), ('д ', 11718), ('й ', 95410), (' у', 68818), (' и', 87593), (' л', 38100), (' о', 129667), (' т', 110817), ('а ', 232963), (' х', 20000), (' ю', 2016), (' е', 58342), ('  ', 79023), (' д', 91764), ('т ', 124094), ('н ', 30095), ('ю ', 32748), (' р', 61120), ('у ', 98515), (' ж', 33652), ('с ', 39193), ('з ', 16816), (' ш', 9285), (' э', 24600), ('г ', 7518), (' ы', 9781), (' ф', 8566), ('ж ', 4946), ('х ', 31854), ('ш ', 1955), ('ц ', 4475), ('п ', 1516), ('р ', 14385), ('б ', 2361), ('ч ', 2967), ('ф ', 635), (' щ', 994), ('щ ', 523), (' й', 1299), (' ь', 320), ('э ', 143), (' ъ', 35), ('ъ ', 2)]\n"
     ]
    }
   ],
   "source": [
    "a = Subsequence_counter(\"\".join(cut_text), max_len=3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 4. RNN\n",
    "## Задача 1. 3 балла\n",
    "Обучите RNN/LSTM на данных из классной работы, используя другой токенайзер. Опишите его и свой выбор. Покажите разницу в генерации моделей, обученных с разными токенайзерами.\n",
    "## {*} Задача 1.1 2 балла\n",
    "Напишите свой токенайзер вручную, с использованием только библиотек numpy, torch, sklearn, stats, опционально других пакетов, не предоставляющих готовые инструменты токенизации и т.п., за исключением предобработки текста (лемматизация, стеминг и т.д.) . \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Tokenizer:\n",
    "    def __init__(self, cut_text, max_len: int = 512):\n",
    "        self.text = text\n",
    "        self.max_len = max_len\n",
    "        self.specials = ['<pad>', '<bos>', '<eos>']\n",
    "        unique_chars = tuple(set(text))\n",
    "        self.int2char = dict(enumerate(tuple(set(text))))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        self._add_special(\"<pad>\")\n",
    "        self._add_special('<bos>')\n",
    "        self._add_special('<eos>')\n",
    "    \n",
    "    def _add_special(self, symbol) -> None:\n",
    "        # add special characters to yuor dicts\n",
    "        sym_num = len(self.char2int)\n",
    "        self.char2int[symbol] = sym_num\n",
    "        self.int2char[sym_num] = symbol\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.int2char) # your code\n",
    "        \n",
    "    def decode_symbol(self, el):\n",
    "        return self.int2char[el]\n",
    "        \n",
    "    def encode_symbol(self, el):\n",
    "        return self.char2int[el]\n",
    "        \n",
    "    def str_to_idx(self, chars):\n",
    "        return [self.char2int[sym] for sym in chars] # str -> list[int]\n",
    "\n",
    "    def idx_to_str(self, idx):\n",
    "        return [self.int2char[toc] for toc in idx] # list[int] -> list[str]\n",
    "\n",
    "    def encode(self, chars, eos=True):\n",
    "        if eos:\n",
    "            chars = ['<bos>'] + list(chars) + ['<eos>']\n",
    "        else:\n",
    "            chars = ['<bos>'] + list(chars)\n",
    "        return self.str_to_idx(chars)\n",
    "\n",
    "    def decode(self, idx):\n",
    "        chars = self.idx_to_str(idx)\n",
    "        return \"\".join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JokesDataset(Dataset):\n",
    "    def __init__(self, tokenizer, cut_text, max_len: int = 256):\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cut_text = cut_text\n",
    "        self.pad_index = self.tokenizer.encode_symbol(\"<pad>\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cut_text)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        #  в идеале запонлять паддингами лучше в другом месте\n",
    "        encoded = self.tokenizer.encode(self.cut_text[item])[:self.max_len]\n",
    "        padded = torch.full((self.max_len, ), self.pad_index, dtype=torch.long)\n",
    "        padded[:len(encoded)] = torch.tensor(encoded)\n",
    "        # pad your sequence and make a final sample. You can skip padding and pad sequences with torch special method.\n",
    "        return padded, len(encoded)\n",
    "\n",
    "# Optionally add new methods to your dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        hidden_dim: int = 256,\n",
    "        num_layers: int = 2,\n",
    "        drop_prob: float = 0.5,\n",
    "        max_len: int = 512,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.max_len = max_len\n",
    "        # create mappings\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        ## define the LSTM, dropout and fully connected layers\n",
    "        self.encoder = nn.Embedding(self.tokenizer.vocab_size, self.hidden_dim)\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.hidden_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=self.drop_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=self.drop_prob)\n",
    "        self.decoder = nn.Linear(\n",
    "            in_features=self.hidden_dim,\n",
    "            out_features=self.tokenizer.vocab_size,\n",
    "        )\n",
    "\n",
    "    # Forward - это проход вперёд по слою\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, lengths: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # one-hot encode your sequence\n",
    "        packed_embeds = self.encoder(x) # pack your sequence. This helps with the efficiency. Use torch function pack_padded_sequence\n",
    "        outputs, hidden = self.rnn(packed_embeds) # run you model\n",
    "        \n",
    "        # TODO: Понять нафига\n",
    "        #  out, lengths = # pad sequence back\n",
    "        \n",
    "        # Pass through a dropout layer and fully connected layer\n",
    "        out = self.dropout(outputs)\n",
    "        ## Get the output for classification.\n",
    "        out = self.decoder(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    \n",
    "    # инференс - режим не обучения (По сути штатная работа)\n",
    "    def inference(self, prefix='<bos> ', device=\"cpu\"):\n",
    "        tokens = torch.tensor([self.tokenizer.encode(prefix, eos=False)], device=device) # encode prefix\n",
    "        \n",
    "        # 2 stopping conditions: reaching max len or getting <eos> token\n",
    "        # Generate sequence iteratively\n",
    "        for _ in range(self.max_len - len(tokens[0])):\n",
    "            # YOUR CODE: generate sequence one by one\n",
    "            # Pass tokens through the embedding layer\n",
    "            logits, hidden = self.forward(tokens, torch.tensor([tokens.size(1)]))\n",
    "            \n",
    "            # Get the last token's logits and sample a token\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            new_token = torch.multinomial(\n",
    "                torch.nn.functional.softmax(next_token_logits, dim=-1), num_samples=1\n",
    "            )\n",
    "\n",
    "            # Append the new token\n",
    "            tokens = torch.cat([tokens, new_token], dim=1)\n",
    "\n",
    "            # Stop if the <eos> token is generated\n",
    "            if new_token.item() == self.tokenizer.encode_symbol(\"<eos>\"):\n",
    "                break\n",
    "        # Decode the token IDs back into a string\n",
    "        return self.tokenizer.decode(tokens.squeeze().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(\n",
    "    model: CharRNN,\n",
    "    train_batch: Tuple[torch.Tensor, torch.Tensor],\n",
    "    vocab_size: int,\n",
    "    criterion: nn.Module,\n",
    "    optimizer,\n",
    "    device=\"cpu\"\n",
    ") -> torch.Tensor:\n",
    "    inputs, lengths = train_batch\n",
    "    inputs = inputs.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "\n",
    "    # Сброс градиентов\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Прямой проход\n",
    "    outputs, _ = model(inputs[:, :-1], lengths)\n",
    "\n",
    "    # Переформатирование выходов и целевых меток для расчета функции потерь\n",
    "    outputs = outputs.view(-1, vocab_size)\n",
    "    targets = inputs[:, 1:].reshape(-1)\n",
    "\n",
    "    # Вычисление функции потерь\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Обратный проход\n",
    "    loss.backward()\n",
    "\n",
    "    # Шаг оптимизации\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(losses):\n",
    "    clear_output()\n",
    "    plt.plot(range(1, len(losses) + 1), losses)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "seq_length = 512\n",
    "n_hidden = 64 #256\n",
    "n_layers = 4 #4\n",
    "drop_prob = 0.1\n",
    "lr = 0.001\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Custom_Tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRNN(tokenizer, hidden_dim=n_hidden, num_layers=n_layers, drop_prob=drop_prob)\n",
    "hidden = None\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = JokesDataset(tokenizer, cut_text, 256)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 5.4382 :   8%|▊         | 100/1242 [00:03<00:36, 31.44it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)) \u001b[38;5;28;01mas\u001b[39;00m prbar:\n\u001b[0;32m----> 7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepoch_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\n",
      "File \u001b[0;32m~/Документы/Першин_Никольская_Нейронные_сети/.myvenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Документы/Першин_Никольская_Нейронные_сети/.myvenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Документы/Першин_Никольская_Нейронные_сети/.myvenv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Документы/Першин_Никольская_Нейронные_сети/.myvenv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:317\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Документы/Першин_Никольская_Нейронные_сети/.myvenv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:171\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(elem) \u001b[38;5;241m==\u001b[39m elem_size \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m it):\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_loss = 0\n",
    "    with tqdm.tqdm(total=len(dataloader)) as prbar:\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            loss = training_step(model, batch, tokenizer.vocab_size, criterion, optimizer, device)\n",
    "            epoch_loss += loss\n",
    "        \n",
    "            if i % 100 == 0:\n",
    "                #print(f'Done {i/len(dataloader) * 100:.2f}%, Loss: {loss:.4f}')\n",
    "                metrics_str = f\"Loss: {round(loss, 4)} \"\n",
    "                #for k, v in metrics_dict.items():\n",
    "                #    metrics_str += f\"{k}: {round(float(v), 4)} \"\n",
    "                prbar.set_description(metrics_str)\n",
    "                prbar.update(100)\n",
    "    \n",
    "    epoch_loss /= len(dataloader)\n",
    "    losses.append(epoch_loss)\n",
    "    \n",
    "    plot_losses(losses)\n",
    "    #torch.save(model.state_dict(), \"rnn.pt\")\n",
    "    torch.save(model, \"rnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharRNN(\n",
       "  (encoder): Embedding(217, 64)\n",
       "  (rnn): LSTM(64, 64, num_layers=4, batch_first=True, dropout=0.1)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (decoder): Linear(in_features=64, out_features=217, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.load_state_dict(torch.load(\"rnn.pt\", weights_only=True))\n",
    "model = torch.load(\"rnn.pt\", weights_only=False)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<bos>Пилите, Шура, пилите. Они ответиться раковали ной!<eos>',\n",
       " '<bos>Пилите, Шура, пилите. Они даже кега кот в сде клак, что.<eos>',\n",
       " '<bos>Пилите, Шура, пилите. Они это это туамова так в этох двах, задав момный умерении себе до буситаной...<eos>',\n",
       " '<bos>Пилите, Шура, пилите. Они Мисно проскерыю.<eos>',\n",
       " '<bos>Пилите, Шура, пилите. Они ей хатьице бвипей тут.<eos>',\n",
       " '<bos>Пилите, Шура, пилите. Они фитум попросил, чко выэто стоят предзабчай любитки жизнь...<eos>',\n",
       " '<bos>Пилите, Шура, пилите. Они женой посадилении, чтоба озежую бритирент кой.- Додать, что аза вобликаюсь, а были!<eos>',\n",
       " '<bos>Пилите, Шура, пилите. Они по извегат семья \"Видетнлюсье быть нов сторого когда сава влучают на порюки отметать...<eos>',\n",
       " '<bos>Пилите, Шура, пилите. Они что мочто развели спрашиу другому захуйчась песму налучкарист этом и фотогрыгней первы?<eos>',\n",
       " '<bos>Пилите, Шура, пилите. Они просыр морожения заечно:- Осазнет, - происял и трабыю светов, видиты, чтобы проходит - я есть?- Памите? - Давай?- Алвришенский кожик, они-то я ли грерьныме?<eos>']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[model.inference(\"Пилите, Шура, пилите. Они \", device=device) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 2. 3 балла\n",
    "Реализуйте с помощью только torch/numpy слой RNN, обучите его на данных из классной работы и, опционально, своих данных. Покажите, что модель обучается\n",
    "## {*} Задача 2.1 +1 балл\n",
    "За реализацию слоев GRU/LSTM/bidirectional RNN, многослойной модели по +1 баллу к базовым (даже если ванильная RNN не реализована)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 3. 1/2/3/4 балла\n",
    "**TBD**: \n",
    "Попробуйте обучить рекуррентную сеть задаче классификации. Вы можете воспользоваться сторонними библиотеками для вашей работы, \n",
    "но модель и основной код должны быть написаны на pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  {*} Задача 4. 5/6/7/8 баллов\n",
    "[ссылка](https://www.kaggle.com/t/b2ef08dc3ddf44f981e2ad186c6c508d)\n",
    "\n",
    "Попробуйте обучить сверточную нейронную сеть задаче детекции людей на изображениях разного стиля. Вы можете воспользоваться сторонними библиотеками для вашей работы. Однако, за неисопользование полностью готовых скриптов обучения (как в классной работе) вы получите дополнительные2 балла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting lxml\n",
      "  Downloading lxml-5.3.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Downloading lxml-5.3.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m700.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, lxml, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.12.3 lxml-5.3.0 soupsieve-2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4 lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /home/dima/Документы/Першин_Никольская_Нейронные_сети/.myvenv/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/dima/Документы/Першин_Никольская_Нейронные_сети/.myvenv/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/dima/Документы/Першин_Никольская_Нейронные_сети/.myvenv/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /home/dima/Документы/Першин_Никольская_Нейронные_сети/.myvenv/lib/python3.12/site-packages (from nltk) (4.67.0)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>мама</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>мыла</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>раму,</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>раму</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>блестела</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  frequency\n",
       "0      мама          1\n",
       "1      мыла          1\n",
       "2     раму,          1\n",
       "3      раму          1\n",
       "4  блестела          1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
