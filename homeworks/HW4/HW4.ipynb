{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Iterable, Tuple\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.distributions.categorical import Categorical\n",
    "import tqdm\n",
    "import re\n",
    "from functools import cmp_to_key\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "with open(\"../../datasets/anek_djvu.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|startoftext|>Друзья мои, чтобы соответствовать вам, я готов сделать над собой усилие и стать лучше. Но тогда и вы станьте немного хуже!\\n\\n<|startoftext|>- Люся, ты все еще хранишь мой подарок?- Да.- Я думал, ты выкинула все, что со мной связано.- Плюшевый мишка не виноват, что ты ебл@н...\\n\\n<|startoftext|>- А вот скажи честно, ты во сне храпишь?- Понятие не имею, вроде, нет. От со'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[118:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбитие на анектоды\n",
    "def cut_data(text):\n",
    "    return text.replace(\"\\n\\n\", \"\").split(\"<|startoftext|>\")[1:]\n",
    "cut_text = cut_data(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Доктор предложил мне пропить железо. Теперь у меня нет машины и гаража...'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "cut_text[random.randint(0, 31000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_symbols = {'\\n', ':', ';', '=', '>', '?', '@', ' ', '!', '\"', '#', '$', '%', '&', '\\'', '*', '+', ',', '\\-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Ё', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Ъ', 'Ы', 'Ь', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё'}\n",
    "\n",
    "allowed_symbols2 = {'\\n', ':', ';', '=', '>', '?', '@', ' ', '!', '\"', '#', '$', '%', '&', '\\'', '*', '+', ',', '\\-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Ё', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Ъ', 'Ы', 'Ь', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё'}\n",
    "\n",
    "allowed_symbols2_rus = {':', ';', '?', '@', ' ', '!', '\"', '#', '$', '%', '*', '+', ',', '\\-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9','Ё', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Ъ', 'Ы', 'Ь', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_MAX_TOKEN_LEN = 5\n",
    "GLOBAL_MAX_TOKEN_COUNT = 253"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 4. RNN\n",
    "## Задача 1. 3 балла\n",
    "Обучите RNN/LSTM на данных из классной работы, используя другой токенайзер. Опишите его и свой выбор. Покажите разницу в генерации моделей, обученных с разными токенайзерами.\n",
    "## {*} Задача 1.1 2 балла\n",
    "Напишите свой токенайзер вручную, с использованием только библиотек numpy, torch, sklearn, stats, опционально других пакетов, не предоставляющих готовые инструменты токенизации и т.п., за исключением предобработки текста (лемматизация, стеминг и т.д.) . \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Tokenizer:\n",
    "    def __init__(self, cut_text, max_count_token: int = 512, max_token_len = 2, allowed_s = allowed_symbols2_rus):\n",
    "        self.allowed_s = allowed_s\n",
    "        self.max_token_len = max_token_len\n",
    "        self.text = cut_text\n",
    "        self.max_count_token = max_count_token\n",
    "        self.specials = {'<pad>':999999, '<bos>':999999, '<eos>':999999}\n",
    "        \n",
    "        self.create_vocab(\"\".join(self.text))\n",
    "        \n",
    "    def token_compare(self, item1:dict, item2:dict):\n",
    "        if ((\" \" in item1[0] or \" \" in item2[0]) and not(item1[0] == \" \" or item2[0] == \" \")):\n",
    "            #print(item1[0], item2[0])\n",
    "            if (\" \" in item1[0] and \" \" in item2[0]):\n",
    "                return 0\n",
    "            elif (\" \" in item1[0]):\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        else:\n",
    "            #if (len(item2[0]) > len(item1[0]) and item2[0])\n",
    "            return item2[1] - item1[1]\n",
    "    \n",
    "    def create_vocab(self, text: str):\n",
    "        regex = r'[^' + \"\".join(self.allowed_s).lower() + ']'\n",
    "        reg = re.compile(regex)\n",
    "        text = reg.sub('', text.replace('ё', 'е'))\n",
    "    \n",
    "        my_dict = {}\n",
    "        i = 1\n",
    "        text_len = len(text)\n",
    "        for index in range(0, text_len - i + 1):\n",
    "            token = text[index:index+i]\n",
    "            if not token in my_dict:\n",
    "                my_dict[token]=9999999\n",
    "    \n",
    "        text = re.sub(r'[^а-я ]', '', text)\n",
    "\n",
    "    \n",
    "        for i in range (2, self.max_token_len+1):\n",
    "            text_len = len(text)\n",
    "            for index in range(0, text_len - i + 1):\n",
    "                token = text[index:index+i]\n",
    "                if token in my_dict:\n",
    "                    my_dict[token]+=1\n",
    "                else:\n",
    "                    my_dict[token]=1\n",
    "                # Assign unique IDs to tokens\n",
    "        \n",
    "        # отсортировали по принципу с пробелом в конец, 1 символ в начало,\n",
    "        #   остальное по частоте, чтобы лего сделать crop\n",
    "        sorted_tokens = dict(sorted(my_dict.items(), key=cmp_to_key(self.token_compare))[:self.max_count_token])\n",
    "        # Сортируем по длине токена, а после по частоте\n",
    "        sorted_tokens.update(self.specials)\n",
    "\n",
    "        sorted_tokens = sorted(sorted_tokens.items(), key=lambda x: (-len(x[0]), -x[1]))\n",
    "\n",
    "        self.token_to_id = {token: idx for idx, (token, _) in enumerate(sorted_tokens)}\n",
    "        self.id_to_token = {idx: token for token, idx in self.token_to_id.items()}\n",
    "    \n",
    "    def _add_special(self, symbol) -> None:\n",
    "        # add special characters to yuor dicts\n",
    "        sym_num = len(self.token_to_id)\n",
    "        self.token_to_id[symbol] = sym_num\n",
    "        self.id_to_token[sym_num] = symbol\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.token_to_id) # your code\n",
    "    \n",
    "    def decode_symbol(self, el):\n",
    "        return self.id_to_token[el]\n",
    "        \n",
    "    def encode_symbol(self, el):\n",
    "        return self.token_to_id[el]\n",
    "\n",
    "    #@property\n",
    "    def tokenize(self, text: str):\n",
    "        \"\"\"\n",
    "        Tokenizes the input text into numbers using the built dictionary.\n",
    "        :param text: Input text to tokenize.\n",
    "        :return: A list of token IDs representing the text.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.token_to_id:\n",
    "            raise ValueError(\"Tokenizer dictionary is empty. Call 'build_dict' first.\")\n",
    "        \n",
    "        tokens = []\n",
    "        i = 0\n",
    "        while i < len(text):\n",
    "            match = None\n",
    "            # Try to find the longest matching token\n",
    "            for size in range(max(self.max_token_len, 5), 0, -1):\n",
    "                if i + size <= len(text):\n",
    "                    substring = text[i:i + size]\n",
    "                    if substring in self.token_to_id:\n",
    "                        match = substring\n",
    "                        break\n",
    "            \n",
    "            if match:\n",
    "                tokens.append(self.token_to_id[match])\n",
    "                i += len(match)  # Move past the matched token\n",
    "            else:\n",
    "                # If no match, handle single characters (fallback)\n",
    "                tokens.append(self.token_to_id[text[i]])\n",
    "                i += 1\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def detokenize(self, token_ids):\n",
    "        \"\"\"\n",
    "        Converts token IDs back into the original text using the dictionary.\n",
    "        :param token_ids: A list of token IDs.\n",
    "        :return: The reconstructed text.\n",
    "        \"\"\"\n",
    "        return ''.join(self.id_to_token[token_id] for token_id in token_ids)\n",
    "    \n",
    "    def encode(self, chars, eos=True):\n",
    "        regex = r'[^' + \"\".join(self.allowed_s) + ']'\n",
    "        reg = re.compile(regex)\n",
    "        chars = reg.sub('', chars.lower().replace('ё', 'е'))\n",
    "\n",
    "        if eos:\n",
    "            chars = '<bos>' + chars + '<eos>'\n",
    "        else:\n",
    "            chars = '<bos>' + chars\n",
    "        return self.tokenize(chars)\n",
    "\n",
    "    def decode(self, idx):\n",
    "        return self.detokenize(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "My_CT = Custom_Tokenizer(cut_text,\n",
    "                 max_count_token=253,\n",
    "                 max_token_len = GLOBAL_MAX_TOKEN_LEN,\n",
    "                 allowed_s = allowed_symbols2_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Token Dictionary:\", My_CT.token_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text1 = \"Хозяин: Великолепный поступок. А ты знаешь, что всего только раз в жизни выпадает влюбленным день, когда все им удается?. И ты прозевал свое счастье. Прощай. Я больше не буду тебе помогать. Нет! Мешать начну изо всех сил.\"\n",
    "test_text2 = \"Правосудие - продано\"\n",
    "\n",
    "tokenized_text = My_CT.encode(test_text1)\n",
    "backed_text =  My_CT.decode(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tokenized_text:\n",
    "    print(My_CT.detokenize([i]), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JokesDataset(Dataset):\n",
    "    def __init__(self, tokenizer, cut_text, max_text_len:int = 512):\n",
    "        self.max_text_len = max_text_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cut_text = cut_text\n",
    "        self.pad_index = self.tokenizer.encode_symbol(\"<pad>\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cut_text)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        #  в идеале запонлять паддингами лучше в другом месте\n",
    "        encoded = self.tokenizer.encode(self.cut_text[item])[:self.max_text_len]\n",
    "        padded = torch.full((self.max_text_len, ), self.pad_index, dtype=torch.long)\n",
    "        padded[:len(encoded)] = torch.tensor(encoded)\n",
    "        # pad your sequence and make a final sample. You can skip padding and pad sequences with torch special method.\n",
    "        return padded, len(encoded)\n",
    "\n",
    "# Optionally add new methods to your dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        hidden_dim: int = 256,\n",
    "        num_layers: int = 2,\n",
    "        drop_prob: float = 0.5,\n",
    "        max_text_len: int = 512, # Максимальная длина сообщения в символах\n",
    "    ) -> None:\n",
    "        '''    hidden_dim: Длинна словаря (количество ожидаемых признаков во входных данных x)\n",
    "               max_len: Максимальная длина сообщения в символах\n",
    "        '''\n",
    "        \n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.max_text_len = max_text_len\n",
    "        # create mappings\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        ## define the LSTM, dropout and fully connected layers\n",
    "        self.encoder = nn.Embedding(self.tokenizer.vocab_size, self.hidden_dim)\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.hidden_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=self.drop_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=self.drop_prob)\n",
    "        self.decoder = nn.Linear(\n",
    "            in_features=self.hidden_dim,\n",
    "            out_features=self.tokenizer.vocab_size,\n",
    "        )\n",
    "\n",
    "    # Forward - это проход вперёд по слою\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, lengths: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # one-hot encode your sequence\n",
    "        packed_embeds = self.encoder(x) # pack your sequence. This helps with the efficiency. Use torch function pack_padded_sequence\n",
    "        outputs, hidden = self.rnn(packed_embeds) # run you model\n",
    "        \n",
    "        # TODO: Понять нафига\n",
    "        #  out, lengths = # pad sequence back\n",
    "        \n",
    "        # Pass through a dropout layer and fully connected layer\n",
    "        out = self.dropout(outputs)\n",
    "        ## Get the output for classification.\n",
    "        out = self.decoder(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    \n",
    "    # инференс - режим не обучения (По сути штатная работа)\n",
    "    def inference(self, prefix='<bos> ', device=\"cpu\"):\n",
    "        tokens = torch.tensor([self.tokenizer.encode(prefix, eos=False)], device=device) # encode prefix\n",
    "        \n",
    "        # 2 stopping conditions: reaching max len or getting <eos> token\n",
    "        # Generate sequence iteratively\n",
    "        for _ in range(self.max_text_len - len(tokens[0])):\n",
    "            # YOUR CODE: generate sequence one by one\n",
    "            # Pass tokens through the embedding layer\n",
    "            logits, hidden = self.forward(tokens, torch.tensor([tokens.size(1)]))\n",
    "            \n",
    "            # Get the last token's logits and sample a token\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            new_token = torch.multinomial(\n",
    "                torch.nn.functional.softmax(next_token_logits, dim=-1), num_samples=1\n",
    "            )\n",
    "\n",
    "            # Append the new token\n",
    "            tokens = torch.cat([tokens, new_token], dim=1)\n",
    "\n",
    "            # Stop if the <eos> token is generated\n",
    "            if new_token.item() == self.tokenizer.encode_symbol(\"<eos>\"):\n",
    "                break\n",
    "        # Decode the token IDs back into a string\n",
    "        return self.tokenizer.decode(tokens.squeeze().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(\n",
    "    model: CharRNN,\n",
    "    train_batch: Tuple[torch.Tensor, torch.Tensor],\n",
    "    vocab_size: int,\n",
    "    criterion: nn.Module,\n",
    "    optimizer,\n",
    "    device=\"cpu\"\n",
    ") -> torch.Tensor:\n",
    "    inputs, lengths = train_batch\n",
    "    inputs = inputs.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "\n",
    "    # Сброс градиентов\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Прямой проход\n",
    "    outputs, _ = model(inputs[:, :-1], lengths)\n",
    "\n",
    "    # Переформатирование выходов и целевых меток для расчета функции потерь\n",
    "    outputs = outputs.view(-1, vocab_size)\n",
    "    targets = inputs[:, 1:].reshape(-1)\n",
    "\n",
    "    # Вычисление функции потерь\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Обратный проход\n",
    "    loss.backward()\n",
    "\n",
    "    # Шаг оптимизации\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(losses):\n",
    "    clear_output()\n",
    "    plt.plot(range(1, len(losses) + 1), losses)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_hidden = 256 # 10+3 #64 #256\n",
    "n_layers = 4 #4\n",
    "drop_prob = 0.1\n",
    "lr = 0.001\n",
    "\n",
    "num_epochs = 3\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Custom_Tokenizer(cut_text,\n",
    "                             max_count_token=GLOBAL_MAX_TOKEN_COUNT,\n",
    "                             max_token_len = GLOBAL_MAX_TOKEN_LEN,\n",
    "                             allowed_s = allowed_symbols2_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRNN(tokenizer, hidden_dim=n_hidden, num_layers=n_layers, drop_prob=drop_prob)\n",
    "hidden = None\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = JokesDataset(tokenizer, cut_text, 256)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_loss = 0\n",
    "    with tqdm.tqdm(total=len(dataloader)) as prbar:\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            loss = training_step(model, batch, tokenizer.vocab_size, criterion, optimizer, device)\n",
    "            epoch_loss += loss\n",
    "        \n",
    "            if i % 100 == 0:\n",
    "                #print(f'Done {i/len(dataloader) * 100:.2f}%, Loss: {loss:.4f}')\n",
    "                metrics_str = f\"Loss: {round(loss, 4)} \"\n",
    "                #for k, v in metrics_dict.items():\n",
    "                #    metrics_str += f\"{k}: {round(float(v), 4)} \"\n",
    "                prbar.set_description(metrics_str)\n",
    "                prbar.update(100)\n",
    "    \n",
    "    epoch_loss /= len(dataloader)\n",
    "    losses.append(epoch_loss)\n",
    "    \n",
    "    plot_losses(losses)\n",
    "    #torch.save(model.state_dict(), \"rnn.pt\")\n",
    "    torch.save(model, \"rnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(\"rnn.pt\", weights_only=True))\n",
    "model = torch.load(\"rnn.pt\", weights_only=False)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[model.inference(\"Пилите, Шура, пилите. Они \", device=device) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 2. 3 балла\n",
    "Реализуйте с помощью только torch/numpy слой RNN, обучите его на данных из классной работы и, опционально, своих данных. Покажите, что модель обучается\n",
    "## {*} Задача 2.1 +1 балл\n",
    "За реализацию слоев GRU/LSTM/bidirectional RNN, многослойной модели по +1 баллу к базовым (даже если ванильная RNN не реализована)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 3. 1/2/3/4 балла\n",
    "**TBD**: \n",
    "Попробуйте обучить рекуррентную сеть задаче классификации. Вы можете воспользоваться сторонними библиотеками для вашей работы, \n",
    "но модель и основной код должны быть написаны на pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  {*} Задача 4. 5/6/7/8 баллов\n",
    "[ссылка](https://www.kaggle.com/t/b2ef08dc3ddf44f981e2ad186c6c508d)\n",
    "\n",
    "Попробуйте обучить сверточную нейронную сеть задаче детекции людей на изображениях разного стиля. Вы можете воспользоваться сторонними библиотеками для вашей работы. Однако, за неисопользование полностью готовых скриптов обучения (как в классной работе) вы получите дополнительные2 балла"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
